[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello all who visit. This blog is intended to showcase my work as a graduate student involved in understanding the Hsp90 chaperone, it’s client-chaperone folding cycle, and client specificity. One of the main goals of my research is to selectively modulate Hsp90 chaperone function to selectively impace its downstream client targets by mutation or allosteric small molecule binders."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BCB 504 My Data Blog",
    "section": "",
    "text": "BCB 520 - The Final Portfolio Post\n\n\nHsp90\n\n\n\n\nPortfolio\n\n\nDataViz\n\n\nNetwork\n\n\nObservable\n\n\nAssignment\n\n\n\n\nResearch\n\n\n\n\n\n\nMay 3, 2023\n\n\nErick Rios\n\n\n\n\n\n\n  \n\n\n\n\nNETWORKS IN OBSERVABLE\n\n\nInteractivity and Animation\n\n\n\n\nPortfolio\n\n\nDataViz\n\n\nNetwork\n\n\nObservable\n\n\nAssignment\n\n\n\n\nCool!\n\n\n\n\n\n\nApr 27, 2023\n\n\nErick Rios\n\n\n\n\n\n\n  \n\n\n\n\nPractice with Network Data\n\n\nNodes and Links and edges and vertices…\n\n\n\n\nPortfolio\n\n\nDataViz\n\n\nNetwork\n\n\niGraph\n\n\nAssignment\n\n\n\n\niGRAPH!\n\n\n\n\n\n\nApr 19, 2023\n\n\nBarrie Robison\n\n\n\n\n\n\n  \n\n\n\n\nPractice with Spatial Data\n\n\nMalaria, Ocean Currents, Baseball…\n\n\n\n\nPortfolio\n\n\nDataViz\n\n\nSpatial\n\n\nGGPlot\n\n\nAssignment\n\n\n\n\nMaps and Spatial Fields are fun!\n\n\n\n\n\n\nApr 10, 2023\n\n\nBarrie Robison\n\n\n\n\n\n\n  \n\n\n\n\nBCB 520 - Midterm Portfolio Post\n\n\nGun violence in the US\n\n\n\n\nMidterm\n\n\nData\n\n\nExploration\n\n\nViolence\n\n\nGuns\n\n\n\n\nWhat trend, if any, in gun violence is there in the US?\n\n\n\n\n\n\nMar 21, 2023\n\n\nErick Rios\n\n\n\n\n\n\n  \n\n\n\n\nASSIGNMENT 5\n\n\nVisualizations for Tabular Data\n\n\n\n\nAssignment\n\n\nDataViz\n\n\nTables\n\n\nScatterplot\n\n\nBarplot\n\n\nPiechart\n\n\n\n\nShould I trade these draft picks for this bag of magic beans…?\n\n\n\n\n\n\nMar 6, 2023\n\n\nBarrie Robison\n\n\n\n\n\n\n  \n\n\n\n\nASSIGNMENT 4\n\n\nMarks and Channels\n\n\n\n\nAssignment\n\n\nDataViz\n\n\nKnowledge\n\n\n\n\nThis post contains knowledge and fuel units for your yeast\n\n\n\n\n\n\nFeb 14, 2023\n\n\nErick Rios\n\n\n\n\n\n\n  \n\n\n\n\nAssignments 2 and 3\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nInsert witty description\n\n\n\n\n\n\nFeb 6, 2023\n\n\nErick Rios\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2023\n\n\nErick Rios\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/A8 network/index.html",
    "href": "posts/A8 network/index.html",
    "title": "Practice with Network Data",
    "section": "",
    "text": "In this assignment, we’ll consider some of the tools and techniques for visualizing network data. Network data is characterized by two unique items that are not found in tabular or spatial data - Nodes and Links. In addition, there is a sub-type of network data that we will consider - Hierarchical or Tree data. Let’s practice a few visualizations to get a feel for how these things work!"
  },
  {
    "objectID": "posts/A8 network/index.html#igraph",
    "href": "posts/A8 network/index.html#igraph",
    "title": "Practice with Network Data",
    "section": "IGRAPH",
    "text": "IGRAPH\nLet’s start with igraph, which is an open source toolset for network analysis. The great thing about igraph is that you can use these tools in R, Python, Mathematica, and C++. It is very flexible and very powerful.\n\nigraph in R\nFirst up, we’ll install R/igraph and load the library (note that I’ve commented out the package installation because I’ve already installed igraph on my machine):\n\n\nCode\n# install.packages(\"igraph\")\nlibrary(igraph)\n\n\nWarning: package 'igraph' was built under R version 4.1.3\n\n\n\nAttaching package: 'igraph'\n\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\n\nThe following object is masked from 'package:base':\n\n    union\n\n\nNow I’m going to walk you through a modified version of the igraph tutorial, which you can find here"
  },
  {
    "objectID": "posts/A8 network/index.html#creating-a-graph",
    "href": "posts/A8 network/index.html#creating-a-graph",
    "title": "Practice with Network Data",
    "section": "Creating a graph",
    "text": "Creating a graph\nigraph offers many ways to create a graph. The simplest one is the function make_empty_graph:\n\n\nCode\ng <- make_empty_graph()\n\n\nThe most common way to create a graph is make_graph, which constructs a network based on specified edges. For example, to make a graph with 10 nodes (numbered 1 to 10) and two edges connecting nodes 1-2 and 1-5:\n\n\nCode\ng <- make_graph(edges = c(1,2, 1,5), n=10, directed = FALSE)\n\n\nWe can print the graph to get a summary of its nodes and edges:\n\n\nCode\ng\n\n\nIGRAPH 74aaa77 U--- 10 2 -- \n+ edges from 74aaa77:\n[1] 1--2 1--5\n\n\nThis means: Undirected Named graph with 10 vertices and 2 edges, with the exact edges listed out. If the graph has a [name] attribute, it is printed as well.\n\n\n\n\n\n\nNote\n\n\n\nsummary does not list the edges, which is convenient for large graphs with millions of edges:\n\n\n\n\nCode\nsummary(g)\n\n\nIGRAPH 74aaa77 U--- 10 2 -- \n\n\nThe same function make_graph can create some notable graphs by just specifying their name. For example you can create the graph that represents the social network of Zachary’s karate club, that shows the friendship between 34 members of a karate club at a US university in the 1970s:\n\n\nCode\ng <- make_graph('Zachary')\n\n\nTo visualize a graph you can use plot:\n\n\nCode\nplot(g)"
  },
  {
    "objectID": "posts/A8 network/index.html#vertex-and-edge-ids",
    "href": "posts/A8 network/index.html#vertex-and-edge-ids",
    "title": "Practice with Network Data",
    "section": "Vertex and edge IDs",
    "text": "Vertex and edge IDs\nVertices and edges have numerical vertex IDs in igraph. Vertex IDs are always consecutive and they start with 1. For a graph with n vertices the vertex IDs are always between 1 and n. If some operation changes the number of vertices in the graphs, e.g. a subgraph is created via induced_subgraph, then the vertices are renumbered to satisfy this criterion.\nThe same is true for the edges as well: edge IDs are always between 1 and m, the total number of edges in the graph.\nIn addition to IDs, vertices and edges can be assigned a name and other attributes. That makes it easier to track them whenever the graph is altered."
  },
  {
    "objectID": "posts/A8 network/index.html#addingdeleting-vertices-and-edges",
    "href": "posts/A8 network/index.html#addingdeleting-vertices-and-edges",
    "title": "Practice with Network Data",
    "section": "Adding/deleting vertices and edges",
    "text": "Adding/deleting vertices and edges\nLet’s continue working with the Karate club graph. To add one or more vertices to an existing graph, use add_vertices:\n\n\nCode\ng <- add_vertices(g, 3)\n\n\nSimilarly, to add edges you can use add_edges:\n\n\nCode\ng <- add_edges(g, edges = c(1,35, 1,36, 34,37))\n\n\nEdges are added by specifying the source and target vertex IDs for each edge. This call added three edges, one connecting vertices 1 and 35, one connecting vertices 1 and 36, and one connecting vertices 34 and 37.\nIn addition to the add_vertices and add_edges functions, the plus operator can be used to add vertices or edges to graph. The actual operation that is performed depends on the type of the right hand side argument:\n\n\nCode\ng <- g + edges(c(1,35, 1,36, 34,37))\n\n\nYou can add a single vertex/edge at a time using add_vertex and add_edge.\nLet us add some more vertices and edges to our graph. In igraph we can use the magrittr package, which provides a mechanism for chaining commands with the operator %\\>%:\n\n\nCode\ng <- g %>% add_edges(edges=c(1,34)) %>% add_vertices(3) %>%\n     add_edges(edges=c(38,39, 39,40, 40,38, 40,37))\ng\n\n\nIGRAPH 74e8dcc U--- 40 86 -- Zachary\n+ attr: name (g/c)\n+ edges from 74e8dcc:\n [1]  1-- 2  1-- 3  1-- 4  1-- 5  1-- 6  1-- 7  1-- 8  1-- 9  1--11  1--12\n[11]  1--13  1--14  1--18  1--20  1--22  1--32  2-- 3  2-- 4  2-- 8  2--14\n[21]  2--18  2--20  2--22  2--31  3-- 4  3-- 8  3--28  3--29  3--33  3--10\n[31]  3-- 9  3--14  4-- 8  4--13  4--14  5-- 7  5--11  6-- 7  6--11  6--17\n[41]  7--17  9--31  9--33  9--34 10--34 14--34 15--33 15--34 16--33 16--34\n[51] 19--33 19--34 20--34 21--33 21--34 23--33 23--34 24--26 24--28 24--33\n[61] 24--34 24--30 25--26 25--28 25--32 26--32 27--30 27--34 28--34 29--32\n[71] 29--34 30--33 30--34 31--33 31--34 32--33 32--34 33--34  1--35  1--36\n+ ... omitted several edges\n\n\nCode\nplot(g)\n\n\n\n\n\nWe now have an undirected graph with 40 vertices and 86 edges. Vertex and edge IDs are always contiguous, so if you delete a vertex all subsequent vertices will be renumbered. When a vertex is renumbered, edges are not renumbered, but their source and target vertices will be. Use delete_vertices and delete_edges to perform these operations. For instance, to delete the edge connecting vertices 1-34, get its ID and then delete it:\n\n\nCode\nget.edge.ids(g, c(1,34))\n\n\n[1] 82\n\n\n\n\nCode\ng <- delete_edges(g, 82)\n\n\nAs an example, to create a broken ring:\n\n\nCode\ng <- make_ring(10) %>% delete_edges(\"10|1\")\nplot(g)\n\n\n\n\n\nThe example above shows that you can also refer to edges with strings containing the IDs of the source and target vertices, connected by a pipe symbol |. \"10|1\" in the above example means the edge that connects vertex 10 to vertex 1. Of course you can also use the edge IDs directly, or retrieve them with the get.edge.ids function:\n\n\nCode\ng <- make_ring(5)\ng <- delete_edges(g, get.edge.ids(g, c(1,5, 4,5)))\nplot(g)"
  },
  {
    "objectID": "posts/A8 network/index.html#constructing-graphs",
    "href": "posts/A8 network/index.html#constructing-graphs",
    "title": "Practice with Network Data",
    "section": "Constructing graphs",
    "text": "Constructing graphs\nIn addition to make_empty_graph, make_graph, and make_graph_from_literal, igraph includes many other function to construct a graph. Some are deterministic, i.e. they produce the same graph each single time, e.g. make_tree:\n\n\nCode\ngraph1 <- make_tree(127, 2, mode = \"undirected\")\nsummary(graph1)\n\n\nIGRAPH 7539fea U--- 127 126 -- Tree\n+ attr: name (g/c), children (g/n), mode (g/c)\n\n\nCode\nplot(graph1)\n\n\n\n\n\nThis generates a regular tree graph with 127 vertices, each vertex having two children. No matter how many times you call make_tree, the generated graph will always be the same if you use the same parameters:\n\n\nCode\ngraph2 <- make_tree(127, 2, mode = \"undirected\")\n\n\n\n\nCode\nidentical_graphs(graph1,graph2)\n\n\n[1] TRUE\n\n\nOther functions generate graphs stochastically, i.e. they produce a different graph each time. For instance sample_grg:\n\n\nCode\ngraph1 <- sample_grg(100, 0.2)\nsummary(graph1)\n\n\nIGRAPH 7576d71 U--- 100 578 -- Geometric random graph\n+ attr: name (g/c), radius (g/n), torus (g/l)\n\n\nCode\nplot(graph1)\n\n\n\n\n\nThis generates a geometric random graph: n points are chosen randomly and uniformly inside the unit square and pairs of points closer to each other than a predefined distance d are connected by an edge. If you generate GRGs with the same parameters, they will be different:\n\n\nCode\ngraph2 <- sample_grg(100, 0.2)\nidentical_graphs(graph1, graph2)\n\n\n[1] FALSE\n\n\nCode\nplot(graph2)\n\n\n\n\n\nA slightly looser way to check if the graphs are equivalent is via isomorphic. Two graphs are said to be isomorphic if they have the same number of components (vertices and edges) and maintain a one-to-one correspondence between vertices and edges, i.e., they are connected in the same way.\n\n\nCode\nisomorphic(graph1, graph2)\n\n\n[1] FALSE\n\n\nChecking for isomorphism can take a while for large graphs (in this case, the answer can quickly be given by checking the degree sequence of the two graphs). identical_graph is a stricter criterion than isomorphic: the two graphs must have the same list of vertices and edges, in exactly the same order, with same directedness, and the two graphs must also have identical graph, vertex and edge attributes."
  },
  {
    "objectID": "posts/A8 network/index.html#setting-and-retrieving-attributes",
    "href": "posts/A8 network/index.html#setting-and-retrieving-attributes",
    "title": "Practice with Network Data",
    "section": "Setting and retrieving attributes",
    "text": "Setting and retrieving attributes\nIn addition to IDs, vertex and edges can have attributes such as a name, coordinates for plotting, metadata, and weights. The graph itself can have such attributes too (e.g. a name, which will show in summary). In a sense, every graph, vertex and edge can be used as an R namespace to store and retrieve these attributes.\nTo demonstrate the use of attributes, let us create a simple social network:\n\n\nCode\ng <- make_graph(~ Alice-Bob:Claire:Frank, Claire-Alice:Dennis:Frank:Esther,\n                George-Dennis:Frank, Dennis-Esther)\n\n\nEach vertex represents a person, so we want to store ages, genders and types of connection between two people (is_formal refers to whether a connection between one person or another is formal or informal, i.e. colleagues or friends). The \\$ operator is a shortcut to get and set graph attributes. It is shorter and just as readable as graph_attr and set_graph_attr.\n\n\nCode\nV(g)$age <- c(25, 31, 18, 23, 47, 22, 50) \nV(g)$gender <- c(\"f\", \"m\", \"f\", \"m\", \"m\", \"f\", \"m\")\nE(g)$is_formal <- c(FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE)\nsummary(g)\n\n\nIGRAPH 75d2572 UN-- 7 9 -- \n+ attr: name (v/c), age (v/n), gender (v/c), is_formal (e/l)\n\n\nV and E are the standard way to obtain a sequence of all vertices and edges, respectively. This assigns an attribute to all vertices/edges at once. Another way to generate our social network is with the use of set_vertex_attr and set_edge_attr and the operator %\\>%:\n\n\nCode\ng <- make_graph(~ Alice-Bob:Claire:Frank, Claire-Alice:Dennis:Frank:Esther,\n                George-Dennis:Frank, Dennis-Esther) %>%\n  set_vertex_attr(\"age\", value = c(25, 31, 18, 23, 47, 22, 50)) %>%\n  set_vertex_attr(\"gender\", value = c(\"f\", \"m\", \"f\", \"m\", \"m\", \"f\", \"m\")) %>%\n  set_edge_attr(\"is_formal\", value = c(FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE))\nsummary(g)\n\n\nTo assign or modify an attribute for a single vertex/edge:\n\n\nCode\nE(g)$is_formal\n\n\n[1] FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n\nCode\nE(g)$is_formal[1] <- TRUE\nE(g)$is_formal\n\n\n[1]  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n\nAttribute values can be set to any R object, but note that storing the graph in some file formats might result in the loss of complex attribute values. Vertices, edges and the graph itself can all be used to set attributes, e.g. to add a date to the graph:\n\n\nCode\ng$date <- c(\"2022-02-11\")\ngraph_attr(g, \"date\")\n\n\n[1] \"2022-02-11\"\n\n\nTo retrieve attributes, you can also use graph_attr, vertex_attr, and edge_attr. To find the ID of a vertex you can use the function match:\n\n\nCode\nmatch(c(\"George\"), V(g)$name)\n\n\n[1] 7\n\n\nTo assign attributes to a subset of vertices or edges, you can use:\n\n\nCode\nV(g)$name[1:3] <- c(\"Alejandra\", \"Bruno\", \"Carmina\")\nV(g)\n\n\n+ 7/7 vertices, named, from 75d2572:\n[1] Alejandra Bruno     Carmina   Frank     Dennis    Esther    George   \n\n\nTo delete attributes:\n\n\nCode\ng <- delete_vertex_attr(g, \"gender\")\nV(g)$gender\n\n\nNULL\n\n\nIf you want to save a graph in R with all the attributes use the R’s standard function dput function and retrieve it later with dget. You can also just save the R workspace and restore it later."
  },
  {
    "objectID": "posts/A8 network/index.html#structural-properties-of-graphs",
    "href": "posts/A8 network/index.html#structural-properties-of-graphs",
    "title": "Practice with Network Data",
    "section": "Structural properties of graphs",
    "text": "Structural properties of graphs\nigraph provides a large set of functions to calculate various structural properties of graphs. It is beyond the scope of this tutorial to document all of them, hence this section will only introduce a few of them for illustrative purposes. We will work on the small social network constructed in the previous section.\nPerhaps the simplest property one can think of is the degree. The degree of a vertex equals the number of edges adjacent to it. In case of directed networks, we can also define in-degree (the number of edges pointing towards the vertex) and out-degree (the number of edges originating from the vertex). igraph is able to calculate all of them using a simple syntax:\n\n\nCode\ndegree(g)\n\n\nAlejandra     Bruno   Carmina     Frank    Dennis    Esther    George \n        3         1         4         3         3         2         2 \n\n\nIf the graph was directed, we would have been able to calculate the in- and out-degrees separately using degree(mode=\"in\") and degree(mode=\"out\"). You can also pass a single vertex ID or a list of vertex IDs to degree if you want to calculate the degrees for only a subset of vertices:\n\n\nCode\ndegree(g, 7)\n\n\nGeorge \n     2 \n\n\n\n\nCode\ndegree(g, v=c(3,4,5))\n\n\nCarmina   Frank  Dennis \n      4       3       3 \n\n\nMost functions that accept vertex IDs also accept vertex names (i.e. the values of the name vertex attribute) as long as the names are unique:\n\n\nCode\ndegree(g, v=c(\"Carmina\", \"Frank\", \"Dennis\"))\n\n\nCarmina   Frank  Dennis \n      4       3       3 \n\n\nIt also works for single vertices:\n\n\nCode\ndegree(g, \"Bruno\")\n\n\nBruno \n    1 \n\n\nA similar syntax is used for most of the structural properties igraph can calculate. For vertex properties, the functions accept a vertex ID, a vertex name, or a list of vertex IDs or names (and if they are omitted, the default is the set of all vertices). For edge properties, the functions accept a single edge ID or a list of edge IDs.\n\nNOTE: For some measures, it does not make sense to calculate them only for a few vertices or edges instead of the whole graph, as it would take the same time anyway. In this case, the functions won’t accept vertex or edge IDs, but you can still restrict the resulting list later using standard operations. One such example is eigenvector centrality (evcent()).\n\nBesides degree, igraph includes built-in routines to calculate many other centrality properties, including vertex and edge betweenness (edge_betweenness) or Google’s PageRank (page_rank) just to name a few. Here we just illustrate edge betweenness:\n\n\nCode\nedge_betweenness(g)\n\n\n[1] 6 6 4 3 4 4 4 2 3\n\n\nNow we can also figure out which connections have the highest betweenness centrality:\n\n\nCode\nebs <- edge_betweenness(g)\nas_edgelist(g)[ebs == max(ebs), ]\n\n\n     [,1]        [,2]     \n[1,] \"Alejandra\" \"Bruno\"  \n[2,] \"Alejandra\" \"Carmina\""
  },
  {
    "objectID": "posts/A8 network/index.html#querying-vertices-and-edges-based-on-attributes",
    "href": "posts/A8 network/index.html#querying-vertices-and-edges-based-on-attributes",
    "title": "Practice with Network Data",
    "section": "Querying vertices and edges based on attributes",
    "text": "Querying vertices and edges based on attributes\n\nSelecting vertices\nImagine that in a given social network, you want to find out who has the largest degree. You can do that with the tools presented so far and the which.max function:\n\n\nCode\nwhich.max(degree(g))\n\n\nCarmina \n      3 \n\n\nAnother example would be to select only vertices that have only odd IDs but not even ones, using the V function:\n\n\nCode\ngraph <- graph.full(n=10)\nonly_odd_vertices <- which(V(graph)%%2==1)\nlength(only_odd_vertices)\n\n\n[1] 5\n\n\nOf course, it is possible to select vertices or edges by positional indices:\n\n\nCode\nseq <- V(graph)[2, 3, 7]\nseq\n\n\n+ 3/10 vertices, from 763f39d:\n[1] 2 3 7\n\n\n\n\nCode\nseq <- seq[1, 3]    # filtering an existing vertex set\nseq\n\n\n+ 2/10 vertices, from 763f39d:\n[1] 2 7\n\n\nSelecting a vertex that does not exist results in an error:\n\n\nCode\nseq <- V(graph)[2, 3, 7, \"foo\", 3.5]\n## Error in simple_vs_index(x, ii, na_ok) : Unknown vertex selected\n\n\nAttribute names can also be used as-is within the indexing brackets of V() and E(). This can be combined with R’s ability to use boolean vectors for indexing to obtain very concise and readable expressions to retrieve a subset of the vertex or edge set of a graph. For instance, the following command gives you the names of the individuals younger than 30 years in our social network:\n\n\nCode\nV(g)[age < 30]$name\n\n\n[1] \"Alejandra\" \"Carmina\"   \"Frank\"     \"Esther\"   \n\n\nOf course, < is not the only boolean operator that can be used for this. Other possibilities include the following:\n\n\n\n\n\n\n\nOperator\nMeaning\n\n\n\n\n==\nThe attribute/property value must be equal to\n\n\n!=\nThe attribute/property value must not be equal to\n\n\n<\nThe attribute/property value must be less than\n\n\n<=\nThe attribute/property value must be less than or equal to\n\n\n>\nThe attribute/property value must be greater than\n\n\n>=\nThe attribute/property value must be greater than or equal to\n\n\n%in%\nThe attribute/property value must be included in\n\n\n\nYou can also create a “not in” operator from %in% using the Negate function:\n\n\nCode\n`%notin%` <- Negate(`%in%`)\n\n\nIf an attribute has the same name as an igraph function, you should be careful as the syntax can become a little confusing. For instance, if there is an attribute named degree that represents the grades of an exam for each person, that should not be confused with the igraph function that computes the degrees of vertices in a network sense:\n\n\nCode\nV(g)$degree <- c(\"A\", \"B\", \"B+\", \"A+\", \"C\", \"A\", \"B\")\nV(g)$degree[degree(g) == 3]\n\n\n[1] \"A\"  \"A+\" \"C\" \n\n\n\n\nCode\nV(g)$name[degree(g) == 3]\n\n\n[1] \"Alejandra\" \"Frank\"     \"Dennis\"   \n\n\n\n\nSelecting edges\nEdges can be selected based on attributes just like vertices. As mentioned above, the standard way to get edges is E. Moreover, there are a few special structural properties for selecting edges.\nUsing .from allows you to filter the edge sequence based on the source vertices of the edges. E.g., to select all the edges originating from Carmina (who has vertex index 3):\n\n\nCode\nE(g)[.from(3)]\n\n\n+ 4/9 edges from 75d2572 (vertex names):\n[1] Alejandra--Carmina Carmina  --Frank   Carmina  --Dennis  Carmina  --Esther \n\n\nOf course it also works with vertex names:\n\n\nCode\nE(g)[.from(\"Carmina\")]\n\n\n+ 4/9 edges from 75d2572 (vertex names):\n[1] Alejandra--Carmina Carmina  --Frank   Carmina  --Dennis  Carmina  --Esther \n\n\nUsing .to filters edge sequences based on the target vertices. This is different from .from if the graph is directed, while it gives the same answer for undirected graphs. Using .inc selects only those edges that are incident on a single vertex or at least one of the vertices, irrespectively of the edge directions.\nThe %--% operator can be used to select edges between specific groups of vertices, ignoring edge directions in directed graphs. For instance, the following expression selects all the edges between Carmina (vertex index 3), Dennis (vertex index 5) and Esther (vertex index 6):\n\n\nCode\nE(g) [ 3:5 %--% 5:6 ]\n\n\n+ 3/9 edges from 75d2572 (vertex names):\n[1] Carmina--Dennis Carmina--Esther Dennis --Esther\n\n\nTo make the %--% operator work with names, you can build string vectors containing the names and then use these vectors as operands. For instance, to select all the edges that connect men to women, we can do the following after re-adding the gender attribute that we deleted earlier:\n\n\nCode\nV(g)$gender <- c(\"f\", \"m\", \"f\", \"m\", \"m\", \"f\", \"m\")\n\n\n\n\nCode\nmen <- V(g)[gender == \"m\"]$name\nmen\n\n\n[1] \"Bruno\"  \"Frank\"  \"Dennis\" \"George\"\n\n\n\n\nCode\nwomen <- V(g)[gender == \"f\"]$name\nwomen\n\n\n[1] \"Alejandra\" \"Carmina\"   \"Esther\"   \n\n\n\n\nCode\nE(g)[men %--% women]\n\n\n+ 5/9 edges from 75d2572 (vertex names):\n[1] Alejandra--Bruno  Alejandra--Frank  Carmina  --Frank  Carmina  --Dennis\n[5] Dennis   --Esther"
  },
  {
    "objectID": "posts/A8 network/index.html#treating-a-graph-as-an-adjacency-matrix",
    "href": "posts/A8 network/index.html#treating-a-graph-as-an-adjacency-matrix",
    "title": "Practice with Network Data",
    "section": "Treating a graph as an adjacency matrix",
    "text": "Treating a graph as an adjacency matrix\nThe adjacency matrix is another way to represent a graph. In an adjacency matrix, rows and columns are labeled by graph vertices, and the elements of the matrix indicate the number of edges between vertices i and j. The adjacency matrix for the example graph is:\n\n\nCode\nget.adjacency(g)\n\n\n7 x 7 sparse Matrix of class \"dgCMatrix\"\n          Alejandra Bruno Carmina Frank Dennis Esther George\nAlejandra         .     1       1     1      .      .      .\nBruno             1     .       .     .      .      .      .\nCarmina           1     .       .     1      1      1      .\nFrank             1     .       1     .      .      .      1\nDennis            .     .       1     .      .      1      1\nEsther            .     .       1     .      1      .      .\nGeorge            .     .       .     1      1      .      .\n\n\nFor example, Carmina (1, 0, 0, 1, 1, 1, 0) is directly connected to Alejandra (who has vertex index 1), Frank (index 4), Dennis (index 5) and Esther (index 6), but not to Bruno (index 2) or to George (index 7)."
  },
  {
    "objectID": "posts/A8 network/index.html#layouts-and-plotting",
    "href": "posts/A8 network/index.html#layouts-and-plotting",
    "title": "Practice with Network Data",
    "section": "Layouts and plotting",
    "text": "Layouts and plotting\nA graph is an abstract mathematical object without a specific representation in 2D, 3D or any other geometric space. This means that whenever we want to visualise a graph, we have to find a mapping from vertices to coordinates in two- or three-dimensional space first, preferably in a way that is useful and/or pleasing for the eye. A separate branch of graph theory, namely graph drawing, tries to solve this problem via several graph layout algorithms. igraph implements quite a few layout algorithms and is also able to draw them onto the screen or to any output format that R itself supports.\n\nLayout algorithms\nThe layout functions in igraph always start with layout. The following table summarises them:\n\n\n\n\n\n\n\nMethod name\nAlgorithm description\n\n\n\n\nlayout_randomly\nPlaces the vertices completely randomly\n\n\nlayout_in_circle\nDeterministic layout that places the vertices on a circle\n\n\nlayout_on_sphere\nDeterministic layout that places the vertices evenly on the surface of a sphere\n\n\nlayout_with_drl\nThe Drl (Distributed Recursive Layout) algorithm for large graphs\n\n\nlayout_with_fr\nFruchterman-Reingold force-directed algorithm\n\n\nlayout_with_kk\nKamada-Kawai force-directed algorithm\n\n\nlayout_with_lgl\nThe LGL (Large Graph Layout) algorithm for large graphs\n\n\nlayout_as_tree\nReingold-Tilford tree layout, useful for (almost) tree-like graphs\n\n\nlayout_nicely\nLayout algorithm that automatically picks one of the other algorithms based on certain properties of the graph\n\n\n\nLayout algorithms can be called directly with a graph as its first argument. They will return a matrix with two columns and as many rows as the number of vertices in the graph; each row will correspond to the position of a single vertex, ordered by vertex IDs. Some algorithms have a 3D variant; in this case they return three columns instead of 2.\n\n\nCode\nlayout <- layout_with_kk(g)\n\n\nSome layout algorithms take additional arguments; e.g., when laying out a graph as a tree, it might make sense to specify which vertex is to be placed at the root of the layout:\n\n\nCode\nlayout <- layout_as_tree(g, root = 2)\n\n\n\n\nDrawing a graph using a layout\nWe can plot our imaginary social network with the Kamada-Kawai layout algorithm as follows:\n\n\nCode\nlayout <- layout_with_kk(g)\n\n\n\n\nCode\nlayout <- layout_randomly(g)\n\n\n\n\nCode\nplot(g, layout = layout, main = \"Social network with the Kamada-Kawai layout algorithm\")\n\n\n\n\n\nThis should open a new window showing a visual representation of the network. Remember that the exact placement of nodes may be different on your machine since the layout is not deterministic.\nThe layout argument also accepts functions; in this case, the function will be called with the graph as its first argument. This makes it possible to just pass the name of a layout function directly, without creating a layout variable:\n\n\nCode\nplot(g, layout = layout_with_fr,\n     main = \"Social network with the Fruchterman-Reingold layout algorithm\")\n\n\n\n\n\nTo improve the visuals, a trivial addition would be to color the vertices according to the gender. We should also try to place the labels slightly outside the vertices to improve readability:\n\n\nCode\nV(g)$color <- ifelse(V(g)$gender == \"m\", \"yellow\", \"red\")\nplot(g, layout = layout, vertex.label.dist = 3.5,\n     main = \"Social network - with genders as colors\")\n\n\n\n\n\nYou can also treat the gender attribute as a factor and provide the colors with an argument to plot(), which takes precedence over the color vertex attribute. Colors will be assigned automatically to levels of a factor:\n\n\nCode\nplot(g, layout=layout, vertex.label.dist=3.5, vertex.color=as.factor(V(g)$gender))\n\n\n\n\n\nAs seen above with the vertex.color argument, you can specify visual properties as arguments to plot instead of using vertex or edge attributes. The following plot shows the formal ties with thick lines while informal ones with thin lines:\n\n\nCode\nplot(g, layout=layout, vertex.label.dist=3.5, vertex.size=20,\n     vertex.color=ifelse(V(g)$gender == \"m\", \"yellow\", \"red\"),\n     edge.width=ifelse(E(g)$is_formal, 5, 1))\n\n\n\n\n\nThis latter approach is preferred if you want to keep the properties of the visual representation of your graph separate from the graph itself.\nIn summary, there are special vertex and edge properties that correspond to the visual representation of the graph. These attributes override the default settings of igraph (i.e color, weight, name, shape,layout,etc.). The following two tables summarise the most frequently used visual attributes for vertices and edges, respectively:\n\n\nVertex attributes controlling graph plots\n\n\n\n\n\n\n\n\nAttribute name\nKeyword argument\nPurpose\n\n\n\n\ncolor\nvertex.color\nColor of the vertex\n\n\nlabel\nvertex.label\nLabel of the vertex. They will be converted to character. Specify NA to omit vertex labels. The default vertex labels are the vertex ids.\n\n\nlabel.cex\nvertex.label.cex\nFont size of the vertex label, interpreted as a multiplicative factor, similarly to R’s text function\n\n\nlabel.color\nvertex.label.color\nColor of the vertex label\n\n\nlabel.degree\nvertex.label.degree\nIt defines the position of the vertex labels, relative to the center of the vertices. It is interpreted as an angle in radian, zero means ‘to the right’, and ‘pi’ means to the left, up is -pi/2 and down is pi/2. The default value is -pi/4\n\n\nlabel.dist\nvertex.label.dist\nDistance of the vertex label from the vertex itself, relative to the vertex size\n\n\nlabel.family\nvertex.label.family\nFont family of the vertex, similarly to R’s text function\n\n\nlabel.font\nvertex.label.font\nFont within the font family of the vertex, similarly to R’s text function\n\n\nshape\nvertex.shape\nThe shape of the vertex, currently “circle”, “square”, “csquare”, “rectangle”, “crectangle”, “vrectangle”, “pie” (see vertex.shape.pie), ‘sphere’, and “none” are supported, and only by the plot.igraph command.\n\n\nsize\nvertex.size\nThe size of the vertex, a numeric scalar or vector, in the latter case each vertex sizes may differ\n\n\n\n\n\nEdge attributes controlling graph plots\n\n\n\n\n\n\n\n\nAttribute name\nKeyword argument\nPurpose\n\n\n\n\ncolor\nedge.color\nColor of the edge\n\n\ncurved\nedge.curved\nA numeric value specifies the curvature of the edge; zero curvature means straight edges, negative values means the edge bends clockwise, positive values the opposite. TRUE means curvature 0.5, FALSE means curvature zero\n\n\narrow.size\nedge.arrow.size\nCurrently this is a constant, so it is the same for every edge. If a vector is submitted then only the first element is used, ie. if this is taken from an edge attribute then only the attribute of the first edge is used for all arrows.\n\n\narrow.width\nedge.arrow.width\nThe width of the arrows. Currently this is a constant, so it is the same for every edge\n\n\nwidth\nedge.width\nWidth of the edge in pixels\n\n\nlabel\nedge.label\nIf specified, it adds a label to the edge.\n\n\nlabel.cex\nedge.label.cex\nFont size of the edge label, interpreted as a multiplicative factor, similarly to R’s text function\n\n\nlabel.color\nedge.label.color\nColor of the edge label\n\n\nlabel.family\nedge.label.family\nFont family of the edge, similarly to R’s text function\n\n\nlabel.font\nedge.label.font\nFont within the font family of the edge, similarly to R’s text function\n\n\n\n\n\nGeneric arguments of plot()\nThese settings can be specified as arguments to the plot function to control the overall appearance of the plot.\n\n\n\n\n\n\n\nKeyword argument\nPurpose\n\n\n\n\nlayout\nThe layout to be used. It can be an instance of Layout, a list of tuples containing X-Y coordinates, or the name of a layout algorithm. The default is auto, which selects a layout algorithm automatically based on the size and connectedness of the graph.\n\n\nmargin\nThe amount of empty space below, over, at the left and right of the plot, it is a numeric vector of length four."
  },
  {
    "objectID": "posts/A8 network/index.html#assignment",
    "href": "posts/A8 network/index.html#assignment",
    "title": "Practice with Network Data",
    "section": "ASSIGNMENT",
    "text": "ASSIGNMENT\nImprove the network above by:\n\nColoring the edges according to Advisor / BCB520 attribute.\nColoring the nodes according to Department.\nAdujsting the labels to improve readability.\n\n\n\nCode\n# Load required packages\nlibrary(igraph)\n\n### Force directed graph of 350 proteins\nstack <- read.csv(\"trimmed_proteomic_Data.csv\")\n\nnodes <- unique(c(stack$contrast, stack$Gene))\nedges <- data.frame(\n  from = stack$Gene,\n  to = stack$contrast,\n  LogFC = stack$logFC,\n  p = stack$P.Value\n  \n)\n\ng <- graph_from_data_frame(edges, directed = FALSE, vertices = nodes)\n\nedge_weights <- abs(E(g)$LogFC)^3\nE(g)$length <- edge_weights\n\nl <- layout_with_fr(g, weights = edge_weights)\n\ndefault_size <- 4\n\nvertex_sizes <- rep(default_size, vcount(g))\n\nmutants <- c(\"A583T\",\"G309S\", \"G424D\",\n             \"K102E\", \"Q380K\", \"R46G\", \n             \"S25P\", \"S481Y\", \"W296A\")\n\nvertex_sizes[V(g)$name %in% mutants] <- 8\n\nV(g)$label.color <- \"black\"\nV(g)$label.font <- 2\n\nplot(g, layout = l, vertex.color = \"white\", vertex.size = vertex_sizes,\n     vertex.label.cex = .25, edge.width = -log10(E(g)$p)*.2, edge.arrow.size = 0, frame.plot = T,  main = \"350\")\n\n\n\n\n\nCode\n###"
  },
  {
    "objectID": "posts/Assignment 2 and 3/index.html",
    "href": "posts/Assignment 2 and 3/index.html",
    "title": "Assignments 2 and 3",
    "section": "",
    "text": "In the quest to ensure data reproducibility, my goals are to:\n\nCollect data of interest.\nImport my data of interest into Rstudio.\nDescribe the data.\nIf time permits, host the data on a repository; probably github.\n\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/Assignment 2 and 3/index.html#how-many-if-any-proteins-had-a-significant-change-in-their-steady-state-levels-in-at-least-one-mutant-condition",
    "href": "posts/Assignment 2 and 3/index.html#how-many-if-any-proteins-had-a-significant-change-in-their-steady-state-levels-in-at-least-one-mutant-condition",
    "title": "Assignments 2 and 3",
    "section": "How many, if any, proteins had a significant change in their steady-state levels in at least one mutant condition?",
    "text": "How many, if any, proteins had a significant change in their steady-state levels in at least one mutant condition?\n\nGene <- unique(Proteomic.Data$Gene[abs(Proteomic.Data$logFC) >= 1.5 &\n                                     Proteomic.Data$P.Value <= 0.055])\n\n\n\nCode\nHits <- paste(length(Gene), \"out of\", length(unique(Proteomic.Data$Gene)), \"proteins were significantly affected in at least one mutant condition; Here we consider any protein whose steady state level changed by at least 1.5 (|Log2FC|)\" )\n\nknitr::kable(Hits)\n\n\n\n\n\n\n\n\nx\n\n\n\n\n350 out of 2482 proteins were significantly affected in at least one mutant condition; Here we consider any protein whose steady state level changed by at least 1.5 (|Log2FC|)\n\n\n\n\n\nConsidering we found these genes of interest using the attributes ‘logFC’ and ‘P.Value’; lets use these attributes to visualize our findings.\n\n\nCode\nx <- Proteomic.Data$logFC\ny <- -log10(Proteomic.Data$P.Value)\n\ngroup <- rep(1, nrow(Proteomic.Data))               # Create group variable\ngroup[abs(x) >= 1.5 & y >= -log10(0.055)] <- 2\ngroup[abs(x) >= 1.5 & y <= -log10(0.055)] <- 3\n\nplot(x, y, \n     col = group, \n     pch = 20,\n     ylim = c(0,25),\n     xlim = c(-6.5, 6.5),\n     xlab = expression(\"Log\"[2]*\"Fold Change\"),\n     ylab = expression(\"-Log\"[10]*\"(P.Value)\")\n)\nabline(v = c(1.5, -1.5), col = \"red\", lty = 2)\nabline(h = -log10(0.055), col = \"red\", lty = 2)\ntext(x = 1.45, y = 24, labels = \"Decreased steady state levels \\n p value < 0.055\",\n     pos = 4, offset = .5)\ntext(x = -1.45, y = 24, labels = \"Increased steady state levels \\n p value < 0.055\",\n     pos = 2, offset = .5)\ntext(x = -4, y = -log10(0.055), labels = \"p value > 0.055\", pos = 1, offset = .5)\ntext(x = 4, y = -log10(0.055), labels = \"p value > 0.055\", pos = 1, offset = .5)\n\n\n\n\n\nWith this ‘volcano’ plot we visualized what was done prior when we searched for any proteins that met our criteria for significance. Those that met our criteria were marked pink and those that did not meet our criteria were marked black or light green. To better visualize the ‘barriers’ for significance, red-dashed lines were added. Proteins with positive logFC values had decreased levels and those with negative logFC values had increased levels. Although not relevant for future analysis, we can spot proteins with large changes in steady state levels, but unfortunately did not meet our threshold for statistical significance (light green) and this is was due to too much variance between the 3 biological replicates in the measurement of that protein."
  },
  {
    "objectID": "posts/Assignment 5/index.html",
    "href": "posts/Assignment 5/index.html",
    "title": "ASSIGNMENT 5",
    "section": "",
    "text": "In this assignment, we are going to practice creating visualizations for tabular data. Unlike previous assignments, however, this time we will all be using the same data sets. I’m doing this because I want everyone to engage in the same logic process and have the same design objectives in mind."
  },
  {
    "objectID": "posts/Assignment 5/index.html#scenario",
    "href": "posts/Assignment 5/index.html#scenario",
    "title": "ASSIGNMENT 5",
    "section": "SCENARIO",
    "text": "SCENARIO\nImagine you are a high priced data science consultant. One of your good friends, Cassandra Canuck, is an Assistant General Manager for the Vancouver Canucks, a team in the National Hockey League with a long, long…. long history of futility.\nCassandra tells you her boss, General Manager Hans Doofenschmirtz, is considering trading this year’s first round draft pick for two second round picks and one third round pick from another team. For the purposes of this exercise, let’s set the 2023 NHL draft order using the Tankathon Simulator. The NHL uses a lottery system in which the teams lowest in the standings have the highest odds of getting the first overall pick. I’ll simulate the lottery now…\nHOLY CRAP! The Vancouver Canucks jump up 6 spots, and will pick FIRST overall. Here is a screenshot:\n\nOur official scenario is this:\nVancouver receives: The 7th pick in the second round (39th overall), the 10th pick in the second round (42nd overall), and the 10th pick in the third round (74th overall).\nDetroit receives: The 1st pick in the first round (1st overall).\nDoofenschmirtz reasons that more draft picks are better, and is inclined to make the trade. Cassandra isn’t so sure…\nShe asks you to create some data visualizations she can show to her boss that might help him make the best decision."
  },
  {
    "objectID": "posts/Assignment 5/index.html#directions",
    "href": "posts/Assignment 5/index.html#directions",
    "title": "ASSIGNMENT 5",
    "section": "DIRECTIONS",
    "text": "DIRECTIONS\nCreate a new post in your portfolio for this assignment. Call it something cool, like NHL draft analysis, or Hockey Analytics, or John Wick….\nCopy the data files from the repository, and maybe also the .qmd file.\nUse the .qmd file as the backbone of your assignment, changing the code and the markdown text as you go."
  },
  {
    "objectID": "posts/Assignment 5/index.html#the-data",
    "href": "posts/Assignment 5/index.html#the-data",
    "title": "ASSIGNMENT 5",
    "section": "THE DATA",
    "text": "THE DATA\nHow can we evaluate whether trading a first round pick for two second round picks and a third round pick is a good idea? One approach is to look at the historical performance of players from these draft rounds.\nI’ve created a data set that will allow us to explore player performance as a function of draft position. If you are curious as to how I obtained and re-arranged these data, you can check out that tutorial here. For this assignment, though, I want to focus on the visualizations.\n\n\nCode\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.5     v purrr   0.3.4\nv tibble  3.1.6     v dplyr   1.0.7\nv tidyr   1.2.0     v stringr 1.4.0\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.1.3\n\n\nCode\nNHLDraft<-read.csv(\"NHLDraft.csv\")\nNHLDictionary<-read_excel(\"NHLDictionary.xlsx\")\nglimpse(NHLDraft)\n\n\nRows: 105,936\nColumns: 12\n$ X           <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,~\n$ draftyear   <int> 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001~\n$ name        <chr> \"Drew Fata\", \"Drew Fata\", \"Drew Fata\", \"Drew Fata\", \"Drew ~\n$ round       <int> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3~\n$ overall     <int> 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86~\n$ pickinRound <int> 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23~\n$ height      <int> 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73~\n$ weight      <int> 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209, 209~\n$ position    <chr> \"Defense\", \"Defense\", \"Defense\", \"Defense\", \"Defense\", \"De~\n$ playerId    <int> 8469535, 8469535, 8469535, 8469535, 8469535, 8469535, 8469~\n$ postdraft   <int> 0, 1, 2, 4, 5, 10, 11, 12, 13, 3, 6, 7, 8, 9, 14, 15, 16, ~\n$ NHLgames    <int> 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0~\n\n\nCode\nknitr::kable(NHLDictionary)\n\n\n\n\n\n\n\n\n\n\nAttribute\nType\nDescription\n\n\n\n\ndraftyear\nOrdinal\nCalendar year in which the player was drafted into the NHL.\n\n\nname\nItem\nFull name of the player.\n\n\nround\nOrdinal\nRound in which the player was drafted (1 to 7).\n\n\noverall\nOrdinal\nOverall draft position of the player (1 to 224)\n\n\npickinRound\nOrdinal\nPosition in which the player was drafted in their round (1 to 32).\n\n\nheight\nQuantitative\nPlayer height in inches.\n\n\nweight\nQuantitative\nPlayer weight in pounds.\n\n\nposition\nCategorical\nPlayer position (Forward, Defense, Goaltender)\n\n\nplayerId\nItem\nUnique ID (key) assigned to each player.\n\n\npostdraft\nOrdinal\nNumber of seasons since being drafted (0 to 20).\n\n\nNHLgames\nQuantitative\nNumber of games played in the NHL in that particular season (regular season is 82 games, playoffs are up to 28 more).\n\n\n\n\n\nIn this case, we have a dataframe with all the drafted players since 2000, their position, their draft year and position, and then rows for each season since being drafted (postdraft). The key variable here is NHLgames, which tells us how many games they played in the NHL each season since being drafted."
  },
  {
    "objectID": "posts/Assignment 5/index.html#simple-scatterplot",
    "href": "posts/Assignment 5/index.html#simple-scatterplot",
    "title": "ASSIGNMENT 5",
    "section": "SIMPLE SCATTERPLOT",
    "text": "SIMPLE SCATTERPLOT\nOne thing to realize about professional hockey is that it is pretty rare for a player to play in the NHL right after being drafted. Players get drafted when they are 18 years old, and they usually play in the juniors, minor leagues, or the NCAA for a bit to further develop. Let’s use a scatterplot to visualize this phenomenon with the most recent draft classes.\n\n\nCode\ndraft2022<-NHLDraft%>%\n  filter(draftyear==2022 & postdraft==0)\n\nggplot(draft2022, aes(x=round, y=NHLgames))+\n  geom_point()\n\n\n\n\n\nAs you can see, the players drafted in June of 2022 haven’t played much this season. There are few things wrong with this visualization, however:\n\nOverplotting. All those points on the y=0 line represent about 32 players each. Can you think of a way that adding extra channels might help?\nLabelling. Can we create a solid figure caption and better axis labels for this figure? In your caption, please specify the task(s) the visualizaiton is intended to facilitate, as well as the marks, channels, and key-value pairs used.\nKey-Value pairs: Looks like we are using “round” as a continuous variable. Can we change this to an ordered factor?\n\nIf one of the issues is the ability to distinguish between players (stacked data points), then why not separate by players? Also turn ‘round’ into a channel: shade/saturation.\n\n\nCode\nggplot(draft2022, aes(x= reorder(name, overall), y=NHLgames)) +\n  geom_point(aes(colour = as.factor(round)))+\n  xlab(\"Eager players\") + ylab(\"# games played in first year\")\n\n\n\n\n\nUnfortunately, there are too many names on the X-axis and this is due to 225 unique player’s name showing up. Maybe we should go back to focusing on the round they were picked, since our ultimate goal to visualize if round picks are a predictor of good performance or not. However, to resolve the issue of over plotting, we can try reduction (or derive?) of data. To keep things simple lets take the average games played by players in each round. That should ensure 1 data point for each round. You can’t have over plotting with only 1 data point per category!\n\n\nCode\nAve.column <- NULL\n\nfor(r in 1:7){\n  Average <- ave(draft2022$NHLgames[draft2022$round == r])[1]\n  Ave.column <- rbind(Ave.column, Average)\n}\n\nDF <- data.frame(Round = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"),\n                 Average = Ave.column\n                 )\n\nggplot(DF, aes(x=Round, y=Average))+\n  geom_col()+\n  xlab(\"Rounds\") + ylab(\"Average # of first year games\")\n\n\n\n\n\nCaption: Interesting based on this plot/data, it appears that 1st round picks, on average, are more likely to play games in there first year. Marks: points Channels: Length Key-value pairs: In this case Round # is a key? and average # games played by players in that round is the value?"
  },
  {
    "objectID": "posts/Assignment 5/index.html#expanded-scatterplot",
    "href": "posts/Assignment 5/index.html#expanded-scatterplot",
    "title": "ASSIGNMENT 5",
    "section": "EXPANDED SCATTERPLOT",
    "text": "EXPANDED SCATTERPLOT\nThe data from the most recent draft isn’t really helpful for our question. Let’s go back in time and use a draft year that has had some time to develop and reach their potential. How about 2018?\n\n\nCode\ndraft2018<-NHLDraft%>%\n  filter(draftyear==2018 & postdraft<6)\n\nggplot(draft2018, aes(x=round, y=NHLgames))+\n  geom_point()\n\n\n\n\n\nHmmm… in addition to the problem of overplotting, we’ve got an additional issue here. We actually have two keys and one attribute. The attribute is NHLgames, and the keys are round and postdraft, but we are only using round.\nPostdraft indicates the number of seasons after being drafted. We have several choices here. We can make a visualization that uses both keys, or we can somehow summarize the data for one of the keys.\nFor example, let’s say we just wanted to know the TOTAL number of NHL games played since being drafted.\n\n\nCode\ndrafttot2018<- draft2018%>%\n  group_by(playerId, round, overall, position, name)%>%\n  summarise(totgames=sum(NHLgames))\n\n\n`summarise()` has grouped output by 'playerId', 'round', 'overall', 'position'.\nYou can override using the `.groups` argument.\n\n\nCode\nggplot(drafttot2018, aes(x=round, y=totgames))+\n  geom_point()\n\n\n\n\n\nFine, I guess, but we still have to deal with overplotting, and think about whether a scatterplot really helps us accomplish our task. For this figure do the following:\n\nOverplotting. All those points on the y=0 line represent about 32 players each. Can you you think of a way that adding extra channels might help?\nLabelling. Can we create a solid figure caption and better axis labels for this figure? In your caption, please specify the task(s) the visualizaiton is intended to facilitate, as well as the marks, channels, and key-value pairs used.\nKey-Value pairs: Looks like we are using “round” as a continuous variable. Can we change this to an ordered factor?"
  },
  {
    "objectID": "posts/Assignment 5/index.html#scatterplot-with-overall-draft-position",
    "href": "posts/Assignment 5/index.html#scatterplot-with-overall-draft-position",
    "title": "ASSIGNMENT 5",
    "section": "SCATTERPLOT WITH OVERALL DRAFT POSITION",
    "text": "SCATTERPLOT WITH OVERALL DRAFT POSITION\nThis approach might yield a better match with the scatterplot idiom. What if we ignore draft round, and use the player’s overall draft position instead?\n\n\nCode\nggplot(drafttot2018, aes(x=overall, y=totgames))+\n  geom_point()\n\n\n\n\n\nFor this figure, address the following:\n\nWe are trying to address the notion of trading a pick from round 1 for picks from round 2 and 3. Add visual channels to this plot that will help us make that decision.\nCreate a caption and better axis labels for this figure.\nWhat if we wanted to use more than just the 2018 draft class?"
  },
  {
    "objectID": "posts/Assignment 5/index.html#scatterplot-summary",
    "href": "posts/Assignment 5/index.html#scatterplot-summary",
    "title": "ASSIGNMENT 5",
    "section": "SCATTERPLOT SUMMARY",
    "text": "SCATTERPLOT SUMMARY\nWe seem to be running into an issue in terms of overplotting. Scatterplots are great, but they work best for two quantitative attributes, and we have a situation with one or two keys and one quantitative attribute. The thing is, scatterplots can be very useful when part of our workflow involves modeling the data in some way. We’ll cover this kind of thing in future assignments, but just a bit of foreshadowing here:\n\n\nCode\nggplot(drafttot2018, aes(x=round, y=totgames))+\n  geom_point()+\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\nAdding the smoothed line doesn’t eliminate the overplotting problem, but it does indicate that it exists. We’ll cover other potential solutions (including Cody’s violin plots!) to this issue later in the course, when we get to the notions of faceting and data reduction."
  },
  {
    "objectID": "posts/Assignment 5/index.html#simple-bar-chart",
    "href": "posts/Assignment 5/index.html#simple-bar-chart",
    "title": "ASSIGNMENT 5",
    "section": "SIMPLE BAR CHART",
    "text": "SIMPLE BAR CHART\nOne of the best ways to deal with overplotting is to use our keys to SEPARATE and ORDER our data. Let’s do that now. I’ll stick with the summarized data for the 2018 draft year for now.\n\n\nCode\nggplot(drafttot2018, aes(x = name, y=totgames))+\n  geom_col()\n\n\n\n\n\nEpic. We now have a bar (column, really) chart with the key being player name, and the attribute being the total number of games played. We’ve SEPARATED the data using the spatial x-axis position channel, and aligned to that axis as well. But this visualization clearly sucks. You need to make it better by:\n\nAdding a visual channel indicating draft round.\nFixing the order of the x axis.\nMaking a caption and better axis labels.\nFixing the values of the x axis labels so they aren’t such a mess.\n\n\n\nCode\nggplot(drafttot2018, aes(x = reorder(name, overall), y=totgames))+\n  geom_col(aes(colour = round)) +\n  xlab(\"Eager Players\") + ylab(\"Total games played since draft\") +\n  theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1, size = .5))"
  },
  {
    "objectID": "posts/Assignment 5/index.html#stacked-bar",
    "href": "posts/Assignment 5/index.html#stacked-bar",
    "title": "ASSIGNMENT 5",
    "section": "STACKED BAR?",
    "text": "STACKED BAR?\nStacked bar charts use two keys and one value. Can we leverage this idiom? Perhaps if we used both round and postdraft as our keys and NHLgames as our value…\nThe idea here is that we might be able to get a sense of the temporal pattern of NHL games after a player is drafted. Do first round picks join the NHL earlier? Do they stay in the NHL longer? That kind of thing.\n\n\nCode\nggplot(draft2018, aes(x = postdraft, y=NHLgames, fill=as.factor(round)))+\n  geom_col(position = \"stack\")\n\n\n\n\n\nThis seems like it has some potential, but it definitely needs some work (by you):\n\nYou know the drill by now. Caption! Labels!\nImprove the color palette.\nDo we really only want data from the 2018 draft class?\nConsider the order of rounds within the stack (glyph). Which round is most important? Change the order within the glyphs to reflect this.\n\n\n\nCode\nmycolors <- c(\"#F20295\", \"#F00000\", \"black\", \"#6C007B\", \"#355C7D\", \"cyan\", \"green\")\n\nggplot(draft2018, aes(x = postdraft, y=NHLgames, fill=as.factor(round)))+\n  geom_col(position = \"stack\") +\n  scale_fill_manual(values = mycolors)"
  },
  {
    "objectID": "posts/Assignment 5/index.html#pie-charts-normalized-bar-charts",
    "href": "posts/Assignment 5/index.html#pie-charts-normalized-bar-charts",
    "title": "ASSIGNMENT 5",
    "section": "PIE CHARTS / NORMALIZED BAR CHARTS",
    "text": "PIE CHARTS / NORMALIZED BAR CHARTS\nWe all know that Pie Charts are rarely a good choice, but let’s look at how to make one here. I’ll eliminate all the players drafted in 2018 who never played an NHL game, leaving us 80 players drafted in that year who made “THE SHOW”. Let’s look at how those 80 players were drafted:\n\n\nCode\nplayedNHL2018 <- drafttot2018%>%\n  filter(totgames>0)\n\nggplot(playedNHL2018, aes(x = \"\", fill = factor(round))) +\n  geom_bar(width = 1) +\n  coord_polar(theta = \"y\")\n\n\n\n\n\nObviously this isn’t great, but can you state why? Write a little critique of this visualizaiton that:\n\nConsiders a player who played hundreds of games over their first five years vs a player who played one game in five years.\nEvaluates the relative value of a second round pick and a third round pick.\n\nNow let’s change this to account for the various years post draft:\n\n\nCode\nseasonplayedNHL2018 <- draft2018%>%\n  filter(NHLgames>0)\n\n\nggplot(seasonplayedNHL2018, aes(x = \"\", fill = factor(round))) +\n  geom_bar(width = 1) +\n  coord_polar(theta = \"y\")+\n  facet_wrap(~postdraft)\n\n\n\n\n\nSeems like there is something to work with here, but let’s compare this to a normalized bar chart:\n\n\nCode\nggplot(draft2018, aes(x = postdraft, y=NHLgames, fill=as.factor(round)))+\n  geom_col(position = \"fill\")\n\n\nWarning: Removed 218 rows containing missing values (geom_col).\n\n\n\n\n\nCan you work with this to make it a useful visualization for your friend, Cassandra Canuck?"
  },
  {
    "objectID": "posts/Assignment 5/index.html#heatmap",
    "href": "posts/Assignment 5/index.html#heatmap",
    "title": "ASSIGNMENT 5",
    "section": "HEATMAP?",
    "text": "HEATMAP?\nCould this be useful?\n\n\nCode\nround1<-NHLDraft%>%\n  filter(round==1)\n\nggplot(round1, aes(y = reorder(name, overall), x = postdraft, fill = NHLgames)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"blue\", high = \"red\")"
  },
  {
    "objectID": "posts/Assignment 5/index.html#other-stuff-to-consider",
    "href": "posts/Assignment 5/index.html#other-stuff-to-consider",
    "title": "ASSIGNMENT 5",
    "section": "OTHER STUFF TO CONSIDER",
    "text": "OTHER STUFF TO CONSIDER\n\nDo these visualizations change as a function of player position?\nIs the number of NHL games played really the best metric to use?"
  },
  {
    "objectID": "posts/Assignment 5/index.html#conclusion",
    "href": "posts/Assignment 5/index.html#conclusion",
    "title": "ASSIGNMENT 5",
    "section": "CONCLUSION",
    "text": "CONCLUSION\nBased on your visualizations, what would you advise regarding this trade proposal? Why?"
  },
  {
    "objectID": "posts/Assignment 9 Interacive/index.html",
    "href": "posts/Assignment 9 Interacive/index.html",
    "title": "NETWORKS IN OBSERVABLE",
    "section": "",
    "text": "The QUARTO documentation on Observable can be found here.\nThe preamble of that document summarizes things nicely:\n\nQuarto includes native support for Observable JS, a set of enhancements to vanilla JavaScript created by Mike Bostock (also the author of D3). Observable JS is distinguished by its reactive runtime, which is especially well suited for interactive data exploration and analysis.\nThe creators of Observable JS (Observable, Inc.) run a hosted service at https://observablehq.com/ where you can create and publish notebooks. Additionally, you can use Observable JS (“OJS”) in standalone documents and websites via its core libraries. Quarto uses these libraries along with a compiler that is run at render time to enable the use of OJS within Quarto documents.\nOJS works in any Quarto document (plain markdown as well as Jupyter and Knitr documents). Just include your code in an {ojs} executable code block.\n\n\n\nI’m going to start by trying to replicate this observable notebook:\n\n\nCode\nviewof graph = {\n  const form = html`<form style=\"font: 12px var(--sans-serif); display: flex; height: 33px; align-items: center;\">\n  <label style=\"margin-right: 1em; display: inline-flex; align-items: center;\">\n    <input type=\"radio\" name=\"radio\" value=\"1\" style=\"margin-right: 0.5em;\" checked> Graph 1\n  </label>\n  <label style=\"margin-right: 1em; display: inline-flex; align-items: center;\">\n    <input type=\"radio\" name=\"radio\" value=\"2\" style=\"margin-right: 0.5em;\"> Graph 2\n  </label>\n  <label style=\"margin-right: 1em; display: inline-flex; align-items: center;\">\n    <input type=\"radio\" name=\"radio\" value=\"3\" style=\"margin-right: 0.5em;\"> Graph 3\n  </label>\n</form>`;\n  const graphs = {1: graph1, 2: graph2, 3: graph3};\n  const timeout = setInterval(() => {\n    form.value = graphs[form.radio.value = (+form.radio.value) % 3 + 1];\n    form.dispatchEvent(new CustomEvent(\"input\"));\n  }, 2000);\n  form.onchange = () => form.dispatchEvent(new CustomEvent(\"input\")); // Safari\n  form.oninput = event => { \n    if (event.isTrusted) clearInterval(timeout), form.onchange = null;\n    form.value = graphs[form.radio.value];\n  };\n  form.value = graphs[form.radio.value];\n  invalidation.then(() => clearInterval(timeout));\n  return form;\n}\n\n\n\n\nchart2 = {\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height]);\n\n  const simulation = d3.forceSimulation()\n      .force(\"charge\", d3.forceManyBody().strength(-1000))\n      .force(\"link\", d3.forceLink().id(d => d.id).distance(200))\n      .force(\"x\", d3.forceX())\n      .force(\"y\", d3.forceY())\n      .on(\"tick\", ticked);\n\n  let link = svg.append(\"g\")\n      .attr(\"stroke\", \"#000\")\n      .attr(\"stroke-width\", 1.5)\n    .selectAll(\"line\");\n\n  let node = svg.append(\"g\")\n      .attr(\"stroke\", \"#fff\")\n      .attr(\"stroke-width\", 1.5)\n    .selectAll(\"circle\");\n\n  function ticked() {\n    node.attr(\"cx\", d => d.x)\n        .attr(\"cy\", d => d.y)\n\n    link.attr(\"x1\", d => d.source.x)\n        .attr(\"y1\", d => d.source.y)\n        .attr(\"x2\", d => d.target.x)\n        .attr(\"y2\", d => d.target.y);\n  }\n\n  // Terminate the force layout when this cell re-runs.\n  invalidation.then(() => simulation.stop());\n\n  return Object.assign(svg.node(), {\n    update({nodes, links}) {\n\n      // Make a shallow copy to protect against mutation, while\n      // recycling old nodes to preserve position and velocity.\n      const old = new Map(node.data().map(d => [d.id, d]));\n      nodes = nodes.map(d => Object.assign(old.get(d.id) || {}, d));\n      links = links.map(d => Object.assign({}, d));\n\n      simulation.nodes(nodes);\n      simulation.force(\"link\").links(links);\n      simulation.alpha(1).restart();\n\n      node = node\n        .data(nodes, d => d.id)\n        .join(enter => enter.append(\"circle\")\n          .attr(\"r\", 8)\n          .attr(\"fill\", d => color(d.id)));\n\n      link = link\n        .data(links, d => `${d.source.id}\\t${d.target.id}`)\n        .join(\"line\");\n    }\n  });\n}\n\n\nupdate = chart2.update(graph)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below shows us how Dr. Bostock creates the basic architecture of an EDGE LIST called links along with NODE LABELS called nodes. Have a close look at the structure. He is setting this up in a heirarchy very similar to a JSON file, which we will examine in EXAMPLE 2.\n\n\nCode\ngraph1 = ({\n  nodes: [\n    {id: \"a\"},\n    {id: \"b\"},\n    {id: \"c\"}\n  ],\n  links: []\n})\n\n\ngraph2 = ({\n  nodes: [\n    {id: \"a\"},\n    {id: \"b\"},\n    {id: \"c\"}\n  ],\n  links: [\n    {source: \"a\", target: \"b\"},\n    {source: \"b\", target: \"c\"},\n    {source: \"c\", target: \"a\"}\n  ]\n})\n\n\ngraph3 = ({\n  nodes: [\n    {id: \"Erick\"},\n    {id: \"Jiyin\"},\n    {id: \"Ronald\"},\n    {id: \"Cody\"},\n    {id: \"Mac\"},\n    {id: \"PC\"},\n  ],\n  links: [\n    {source: \"Erick\", target: \"PC\"},\n    {source: \"Jiyin\", target: \"PC\"},\n    {source: \"Jiyin\", target: \"Mac\"},\n    {source: \"Ronald\", target: \"PC\"},\n    {source: \"Cody\", target: \"PC\"},\n    {source: \"Cody\", target: \"Mac\"}\n  ]\n})\n\n\ncolor = d3.scaleOrdinal(d3.schemeTableau10)\n\nheight = 400\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChange the graph3 object so that it contains six nodes called “Jiyin”, “Ronald”, “Erick”, “Cody”, “Mac”, and “PC”. Change the links object to reflect our shared understanding of those links.\n\n\n\nI honestly cannot believe this works! I mean… Hey! Look at this cool interactive network!\n\n\nCode\nchart = ForceGraph(miserables, {\n  nodeId: d => d.id,\n  nodeGroup: d => d.group,\n  nodeTitle: d => `${d.id}\\n${d.group}`,\n  linkStrokeWidth: l => Math.sqrt(l.value),\n  width,\n  height: 600,\n  invalidation // a promise to stop the simulation when the cell is re-run\n})\n\n\n\n\n\n\n\n\n\nThe first line of code in the chunk below defines the data object from a .json file called miserables.json. Have a look at this file within RStudio. Does the overall structure look familiar?\nCould we possibly replace the stupid data file about a stupid musical with something of our own design???\n\n\nCode\nmiserables = FileAttachment(\"miserables.json\").json()\n\n\n// Copyright 2021 Observable, Inc.\n// Released under the ISC license.\n// https://observablehq.com/@d3/force-directed-graph\nfunction ForceGraph({\n  nodes, // an iterable of node objects (typically [{id}, …])\n  links // an iterable of link objects (typically [{source, target}, …])\n}, {\n  nodeId = d => d.id, // given d in nodes, returns a unique identifier (string)\n  nodeGroup, // given d in nodes, returns an (ordinal) value for color\n  nodeGroups, // an array of ordinal values representing the node groups\n  nodeTitle, // given d in nodes, a title string\n  nodeFill = \"currentColor\", // node stroke fill (if not using a group color encoding)\n  nodeStroke = \"#fff\", // node stroke color\n  nodeStrokeWidth = 1.5, // node stroke width, in pixels\n  nodeStrokeOpacity = 1, // node stroke opacity\n  nodeRadius = 5, // node radius, in pixels\n  nodeStrength,\n  linkSource = ({source}) => source, // given d in links, returns a node identifier string\n  linkTarget = ({target}) => target, // given d in links, returns a node identifier string\n  linkStroke = \"#999\", // link stroke color\n  linkStrokeOpacity = 0.6, // link stroke opacity\n  linkStrokeWidth = 1.5, // given d in links, returns a stroke width in pixels\n  linkStrokeLinecap = \"round\", // link stroke linecap\n  linkStrength,\n  colors = d3.schemeTableau10, // an array of color strings, for the node groups\n  width = 640, // outer width, in pixels\n  height = 400, // outer height, in pixels\n  invalidation // when this promise resolves, stop the simulation\n} = {}) {\n  // Compute values.\n  const N = d3.map(nodes, nodeId).map(intern);\n  const LS = d3.map(links, linkSource).map(intern);\n  const LT = d3.map(links, linkTarget).map(intern);\n  if (nodeTitle === undefined) nodeTitle = (_, i) => N[i];\n  const T = nodeTitle == null ? null : d3.map(nodes, nodeTitle);\n  const G = nodeGroup == null ? null : d3.map(nodes, nodeGroup).map(intern);\n  const W = typeof linkStrokeWidth !== \"function\" ? null : d3.map(links, linkStrokeWidth);\n  const L = typeof linkStroke !== \"function\" ? null : d3.map(links, linkStroke);\n\n  // Replace the input nodes and links with mutable objects for the simulation.\n  nodes = d3.map(nodes, (_, i) => ({id: N[i]}));\n  links = d3.map(links, (_, i) => ({source: LS[i], target: LT[i]}));\n\n  // Compute default domains.\n  if (G && nodeGroups === undefined) nodeGroups = d3.sort(G);\n\n  // Construct the scales.\n  const color = nodeGroup == null ? null : d3.scaleOrdinal(nodeGroups, colors);\n\n  // Construct the forces.\n  const forceNode = d3.forceManyBody();\n  const forceLink = d3.forceLink(links).id(({index: i}) => N[i]);\n  if (nodeStrength !== undefined) forceNode.strength(nodeStrength);\n  if (linkStrength !== undefined) forceLink.strength(linkStrength);\n\n  const simulation = d3.forceSimulation(nodes)\n      .force(\"link\", forceLink)\n      .force(\"charge\", forceNode)\n      .force(\"center\",  d3.forceCenter())\n      .on(\"tick\", ticked);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [-width / 2, -height / 2, width, height])\n      .attr(\"style\", \"max-width: 100%; height: auto; height: intrinsic;\");\n\n  const link = svg.append(\"g\")\n      .attr(\"stroke\", typeof linkStroke !== \"function\" ? linkStroke : null)\n      .attr(\"stroke-opacity\", linkStrokeOpacity)\n      .attr(\"stroke-width\", typeof linkStrokeWidth !== \"function\" ? linkStrokeWidth : null)\n      .attr(\"stroke-linecap\", linkStrokeLinecap)\n    .selectAll(\"line\")\n    .data(links)\n    .join(\"line\");\n\n  const node = svg.append(\"g\")\n      .attr(\"fill\", nodeFill)\n      .attr(\"stroke\", nodeStroke)\n      .attr(\"stroke-opacity\", nodeStrokeOpacity)\n      .attr(\"stroke-width\", nodeStrokeWidth)\n    .selectAll(\"circle\")\n    .data(nodes)\n    .join(\"circle\")\n      .attr(\"r\", nodeRadius)\n      .call(drag(simulation));\n\n  if (W) link.attr(\"stroke-width\", ({index: i}) => W[i]);\n  if (L) link.attr(\"stroke\", ({index: i}) => L[i]);\n  if (G) node.attr(\"fill\", ({index: i}) => color(G[i]));\n  if (T) node.append(\"title\").text(({index: i}) => T[i]);\n  if (invalidation != null) invalidation.then(() => simulation.stop());\n\n  function intern(value) {\n    return value !== null && typeof value === \"object\" ? value.valueOf() : value;\n  }\n\n  function ticked() {\n    link\n      .attr(\"x1\", d => d.source.x)\n      .attr(\"y1\", d => d.source.y)\n      .attr(\"x2\", d => d.target.x)\n      .attr(\"y2\", d => d.target.y);\n\n    node\n      .attr(\"cx\", d => d.x)\n      .attr(\"cy\", d => d.y);\n  }\n\n  function drag(simulation) {    \n    function dragstarted(event) {\n      if (!event.active) simulation.alphaTarget(0.3).restart();\n      event.subject.fx = event.subject.x;\n      event.subject.fy = event.subject.y;\n    }\n    \n    function dragged(event) {\n      event.subject.fx = event.x;\n      event.subject.fy = event.y;\n    }\n    \n    function dragended(event) {\n      if (!event.active) simulation.alphaTarget(0);\n      event.subject.fx = null;\n      event.subject.fy = null;\n    }\n    \n    return d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended);\n  }\n\n  return Object.assign(svg.node(), {scales: {color}});\n}\n\n\nimport {howto} from \"@d3/example-components\"\n\nimport {Swatches} from \"@d3/color-legend\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat if we replaced the datafile by making our own json file??\n\n\nCode\nlibrary(jsonlite)\n\n\nWarning: package 'jsonlite' was built under R version 4.1.3\n\n\nCode\n# create data frames for nodes and links\nnodes <- data.frame(\n  id = c(\"Barrie\", \"Ronald\", \"Cody\", \"Erick\", \"Jiyin\", \"Cthulhu\"),\n  group = c(1, 1, 1 , 2, 2, 3)\n)\n\nlinks <- data.frame(\n  source = c(\"Barrie\", \"Ronald\", \"Cody\", \"Barrie\", \"Erick\", \"Jiyin\", \"Ronald\"),\n  target = c(\"Cthulhu\", \"Erick\", \"Jiyin\", \"Erick\", \"Cthulhu\", \"Ronald\", \"Cody\"),\n  value = c(1, 8, 10, 6, 1, 1, 1)\n)\n\n# convert data frames to JSON objects\nnodes_json <- toJSON(list(nodes = nodes), pretty = TRUE)\nlinks_json <- toJSON(list(links = links), pretty = TRUE)\n\n# merge JSON objects into one\njson <- paste0( nodes_json, links_json)\n\n# write JSON object to file\n# write(json, file = \"network_graph.json\")\n\n\n\n\nCode\nnetwork = FileAttachment(\"network_graph.json\").json()\n\nchart10 = ForceGraph(network, {\n  nodeId: d => d.id,\n  nodeGroup: d => d.group,\n  nodeTitle: d => `${d.id}\\n${d.group}`,\n  linkStrokeWidth: l => Math.sqrt(l.value),\n  width,\n  height: 600,\n  invalidation // a promise to stop the simulation when the cell is re-run\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOh god…. now go back and point the stuff to the stuff…\nAnyway…. here is where I want to go:\nAMAZING"
  },
  {
    "objectID": "posts/BCB 520 - Midterm Portfolio Post/index.html",
    "href": "posts/BCB 520 - Midterm Portfolio Post/index.html",
    "title": "BCB 520 - Midterm Portfolio Post",
    "section": "",
    "text": "This midterm assignment I will be doing an exploratory analysis on gun violence in the US."
  },
  {
    "objectID": "posts/BCB 520 - Midterm Portfolio Post/index.html#preamble",
    "href": "posts/BCB 520 - Midterm Portfolio Post/index.html#preamble",
    "title": "BCB 520 - Midterm Portfolio Post",
    "section": "Preamble",
    "text": "Preamble\nPundits are tasked with providing unbiased reporting on new information, as it develops to inform their audiences, in order for society to adapt or adjust to the developing situation. To aid in this effort, many institutions or groups exist to aggregate data on various topics of interest such as gun violence and is often widely available for academics or internet sleuths to analyze, visualize, and interpret. The culmination of these efforts is often a report to answer pertinent questions to topics societies are concerned with.\nHere in the U.S. any citizen has the right to own firearms. However, the proliferation of firearms has many concerned on public safety and crimes committed with firearms. From your average Joe to politicians at the federal level, opinions differ on the threats posed and consequences of wide-spread availability of firearms, ability to conceal or openly carry a firearm. The the advent of social media has increased the flow of information and based on increased reporting of gun related crimes, the topic of firearms back into the forefront of average citizens and policy-makers. However, some people consider the increased reporting on crimes a product of increased flow of information and could mislead society and policy makers into believing crime, in particular gun crime is out of control.\nTherefore in an effort to inform those concerned on the topic of firearms, I will present and interpret the data set described below in an intuitive and discernible manner. The main overarching question that will be addressed is:\nHas there been an increase or decrease in gun violence?\nDefinition: Here I consider a change in the number of casualties, as a measure of increasing/decreasing gun violence."
  },
  {
    "objectID": "posts/BCB 520 - Midterm Portfolio Post/index.html#data",
    "href": "posts/BCB 520 - Midterm Portfolio Post/index.html#data",
    "title": "BCB 520 - Midterm Portfolio Post",
    "section": "Data",
    "text": "Data\nFigures made and presented in this post will be based mainly on a U.S. Gun Violence Data set containing gun violence incidents, retrieved from Kaggle.com on 3/21/2023. However, the data set was originally collected from gunviolencearchive.org.\nTo help the reader understand the keys and attributes available in the data set, an example of the data is displayed and each column header is defined to ensure understanding what values and observations mean.\n\n\nCode\n#|echo: false \nlibrary(tidyverse)\n\n\n\n\nCode\ngundata <- read.csv(\"Gun violence data 1.csv\", stringsAsFactors = FALSE)\nglimpse(gundata)\n\n\nRows: 452,787\nColumns: 7\n$ Incident_ID    <int> 92114, 92117, 92119, 92125, 92129, 92133, 92135, 92137,~\n$ Incident_Date  <chr> \"1/1/2014\", \"1/1/2014\", \"1/1/2014\", \"1/1/2014\", \"1/1/20~\n$ State_Code     <chr> \"KY\", \"KY\", \"KY\", \"OK\", \"OK\", \"NY\", \"NY\", \"NY\", \"NY\", \"~\n$ City_Or_County <chr> \"Lexington\", \"Cynthiana\", \"Louisville\", \"Lawton\", \"Okmu~\n$ Address        <chr> \"Sixth St and Elm Tree Ln\", \"US 62\", \"S 38th St and W B~\n$ Killed         <int> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0~\n$ Injured        <int> 1, 1, 1, 0, 2, 1, 0, 1, 1, 1, 1, 3, 1, 0, 1, 0, 0, 1, 1~\n\n\nCode\ngundata_attributes <- read.csv(\"Gun_violence_data_descriptions.csv\")\nknitr::kable(gundata_attributes)\n\n\n\n\n\n\n\n\n\n\nColumn.names\nData.Type\nDescription\n\n\n\n\nIncident ID\nItem\nUnique identifier for each incident\n\n\nIncident Date\nAttribute\nThe month, day, and year the incident occurred\n\n\nState_Code\nPosition\n2 letter code representing states where incident occurred\n\n\nCity_or_County\nPosition\nThe city or county where an incident occurred\n\n\nAddress\nPosition\nPhysical location within a city or county where an incident occurred\n\n\nKilled\nAttribute\nNumber of people killed in the incident\n\n\nInjured\nAttribute\nNumber of people injured in the incident"
  },
  {
    "objectID": "posts/BCB 520 - Midterm Portfolio Post/index.html#visualizations",
    "href": "posts/BCB 520 - Midterm Portfolio Post/index.html#visualizations",
    "title": "BCB 520 - Midterm Portfolio Post",
    "section": "Visualizations",
    "text": "Visualizations\nWe are working with 1 key that identifies each separate gun incident, the dates when incidents occurred, and several position-type data where the incident takes place.\nGiven we are interested in whether there has been an increase in gun violence/crimes within the recent years, it seems only natural to derive total the number of incidents for each year, across the US.\n\n\nCode\ngundata <- gundata %>%\n  separate(Incident_Date, sep=\"/\", into = c(\"month\", \"day\", \"year\"))\n\ntotal_by_year <- summarise(group_by(gundata, Year = year), Incidents = n())\n\nggplot(total_by_year, aes(x= Year, y= Incidents, group = 1)) +\n  geom_line() + geom_point()\n\n\n\n\n\nInterestingly, from the year 2014 there was an increase of gun incidents up until 2020, then there was a sharp decline in gun violence to levels less than 2014. Although all violence is tragic, fortunately not every incident results in casualties, but injuries or sometimes neither! Compared to 2014 the amount of gun violence appears to be dropping, but for the incidents that do occur: are more people dying, or getting injured? Lets have a look at whether there was an increase in either deaths, injuries or neither.\n\n\nCode\nTotals <- total_by_year\n\nBoth_by_year <- summarise(group_by(gundata[gundata$Killed >= 1 & gundata$Injured >= 1,], Year = year), Incidents = n())\n\nCasualties_by_year <- summarise(group_by(gundata[gundata$Killed >= 1 & gundata$Injured == 0,], Year = year), Incidents = n())\n\nInjuries_by_year <- summarise(group_by(gundata[gundata$Killed == 0 & gundata$Injured >= 1,], Year = year), Incidents = n())\n\nneither_by_year <- summarise(group_by(gundata[gundata$Killed == 0 & gundata$Injured == 0,], Year = year), Incidents = n())\n\n\nTotals <- bind_rows(Totals, Casualties_by_year, Injuries_by_year, neither_by_year, Both_by_year)\nTotals$Type <- c(rep_len(\"Total Incidents\", 9), rep(\"Death\", 9), rep(\"Injured\", 9), rep(\"Neither\", 9), rep(\"Death and Injuries\", 9))\n\n\nggplot(Totals, aes(x= Year, y= Incidents, group = Type, color = Type)) +\n  geom_line() + geom_point()\n\n\n\n\n\nThis is interesting, but perhaps worrying as the data first suggested the U.S. was experiencing less gun violence nationwide. Despite, the total number of violent gun incidents drastically decreased between the years 2020 and 2022, to underneath 2014 levels. However, the amount of deaths and/or injuries per year increased, or at the very least stayed constant! The apparent decrease in gun incidents is a result of non-casualty incidents decreasing.\nEven though there is an apparent decrease in gun violence incidents, it appears the likelihood of a death and/or injury occurring per incident is increasing each year. Could we possibly reflect that by introducing a new channel like size of each point based on the average number of deaths and/or injuries per incident for each year?\n\n\nCode\nM_KIT_per_year <- gundata %>% \n  group_by(year) %>%\n  summarise(mean = mean(c(Killed, Injured)))\n\nM_K_per_year <- gundata %>% \n  group_by(year) %>%\n  summarise(mean = mean(Killed))\n\nM_I_per_year <- gundata %>%\n  group_by(year) %>%\n  summarise(mean = mean(Injured))\n\nNeither <- rep(0, 9)\n\nM_KI_per_year <- gundata %>% \n  group_by(year) %>%\n  summarise(mean = mean(c(Killed, Injured)))\n\nMean <- c(M_KIT_per_year$mean, M_K_per_year$mean, M_I_per_year$mean, Neither, M_KI_per_year$mean)\n\nTotals$Mean <- Mean\n\nggplot(Totals, aes(x= Year, y= Incidents, group = Type, color = Type)) +\n  geom_line() + geom_point(aes(size = Mean)) +\n  scale_size(range = c(1, 10), breaks = c(0, .50, .75, .9))\n\n\n\n\n\nAlthough we were able to visually observe some interesting details about the data using line scatter plot idiom, I believe we have reached a limit. We could continue to add additional channels to convey more information, but the last addition of point size to convey the average number of deaths/injuries per incident does not work very well. This due to the average number of deaths or injuries per year being fairly constant and outliers have negligible impact on the point size when there are 40000+ incidents with only death or injury each year.\nLet us try to stacked bar plot idiom, I think it will be an improvement over the scatter plot describe each category as parts of a whole.\n\n\nCode\nggplot(Totals[10:45,], aes(x = Year, y= Incidents, fill=as.factor(Type)))+\n  geom_col(position = \"stack\")+\n  labs(fill = \"Casualty Type\") +\n  geom_text(aes(label = Incidents), position = position_stack(vjust = .5))\n\n\n\n\n\nCode\n# ggplot(Totals[10:45,], aes(x = Year, y= Incidents, fill=as.factor(Type)))+\n#   geom_col(position = \"fill\") +\n#   labs(fill = \"Casualty Type\")\n\n\nBetween the years 2014 and 2022, we can more clearly observe a decrease in total incidents, however the amount of incidents resulting in death, injury, or both increased when comparing 2014 and 2022 specifically. Despite the apparent down trend in total incidents, it appears the each incident that does occur is more likely to result in at least one casualty.\nAlthough no new information was added based on the scatter plot, the stacked bar plot idiom seems to be better, even if only slightly. Using length on the Y axis to represent the total incidents per year is better than having it be its own category on the scatter plot, in theory this should reduce the cognitive demands on the reader. Also, the connecting lines are not necessary to expressing the order of time.\nIf you recall, we have position data in the form of States, down to the county level. The U.S. is a very large and diverse country, are the trends observed so far applicable to the entire country? or does each state have its own ‘pattern’ of gun violence?\n[Long story short I tried really hard to utilize the county data, but after a few hours I had to throw in the towel! I even had help from ChatGPT, Stack overflow, Github documentation on relevant packages, and I even ended up signing up with Google Maps API and no avail. I will just make things simpler for myself and stick to State level.]\nLet us keep things simple and start by summarizing the entire gun violence data set from the years 2014 to 2022 by summing up all the gun deaths and injuries together as ‘casualties’ and utilize the State-level position data.\n\n\nCode\nlibrary(urbnmapr)\n\nCasualties_by_state_all_data <- gundata %>% \n  group_by(State_Code) %>%\n  summarise(sum = sum(c(Killed, Injured)))\n\ncolnames(Casualties_by_state_all_data)[colnames(Casualties_by_state_all_data) == \"State_Code\"] <- \"state_abbv\"\n\nspatial_data <- left_join(get_urbn_map(map = \"states\", sf = TRUE),\n                          Casualties_by_state_all_data,\n                          by = \"state_abbv\")\n\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\nCode\nggplot() +\n  geom_sf(spatial_data,\n          mapping = aes(fill = sum),\n          color = \"#ffffff\", size = 0.25) +\n  labs(fill = \"All Casualties 2014 to 2022\")\n\n\n\n\n\nWell that is interesting, it appears that the vast majority of casualties are clearly occurring in California, Texas, and Illinois, with runner-ups being Pennsylvania, New York, and Florida.\nAlbeit interesting, it is important to remember a few things.\n\nThe above-mentioned states are the most populous states in the country. Logically, more incidents occur (thus more casualties) where there are more people present. Vice-versa, less populated places will result in less crime due to less people interacting and resulting in less altercations. Unfortunately, that is a limitation of the dataset, as it does not include population size or density to normalize between states.\n\n[After spending so much time trying to work with county level data, I could not be bothered to find a whole new data set and add more complexity to this assignment!]\n\nThis does not address the main question of whether there has been an increase/decrease in gun violence across the entire country or only some states.\n\nAgain, to keep things simple, since we care if there was an overall increase/decreases in gun violence, lets derive a new attribute: “Ratio”. Where Ratio is the ratio of casualties between the years 2022 and 2014 (2022/2014).\n\n\nCode\n# Filter for 2014 and 2022 and group by state\nratio <- gundata %>%\n  filter(year %in% c(2014, 2022)) %>%\n  group_by(State_Code) %>%\n  # Sum killed and injured for each year and calculate ratio\n  summarize(casualties_2014 = sum(c(Killed[year == 2014], Injured[year == 2014])),\n            casualties_2022 = sum(c(Killed[year == 2022], Injured[year == 2022])),\n            ratio = casualties_2022 / casualties_2014)\n\ncolnames(ratio)[colnames(ratio) == \"State_Code\"] <- \"state_abbv\"\n\nspatial_data <- left_join(get_urbn_map(map = \"states\", sf = TRUE),\n                          ratio,\n                          by = \"state_abbv\")\n\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\nCode\nggplot() +\n  geom_sf(spatial_data,\n          mapping = aes(fill = ratio),\n          color = \"#ffffff\", size = 0.5) +\n  labs(fill = \"Ratio (2022/2014)\") +\n  ggtitle(\"Ratio of 2022:2014 Casualties\") + \n  geom_sf_text(data = get_urbn_labels(map = \"states\", sf = TRUE), \n                aes(label = state_abbv), \n            size = 3)+\n  theme(panel.grid = element_blank()) +\n  labs(x = NULL, y = NULL) + \n  coord_sf(datum = NA) +   scale_fill_gradientn(colors = c(\"green\", \"red\"))\n\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n\n\n\nAlthough we do not have information to deal with population density issue, by taking the ratio of casualties in 2022 to 2014, we can clearly see that the states in red/orange (Minnesota, Colorado, New Mexico) had ratios of at least a 2, suggesting a 2 fold increase in gun related casualties. The greener states (For example, Florida, Nebraska, California) had ratios of less than 1, suggesting a decrease in gun casualties."
  },
  {
    "objectID": "posts/BCB 520 - Midterm Portfolio Post/index.html#summary",
    "href": "posts/BCB 520 - Midterm Portfolio Post/index.html#summary",
    "title": "BCB 520 - Midterm Portfolio Post",
    "section": "Summary",
    "text": "Summary\nWhen only taking into account the number of isolated incidents, there has been a sharp decrease in gun violence across the entire U.S.\nHowever, when incidents are separated by whether death, injury, both, or neither occurred, the scatter plot and stacked bar plot idioms revealed the decrease in total incidents is mostly due to non-casualty incidents plummeting, while death, injury, or both increased (or at least stayed constant).\nWhen breaking down the data by state, we observed initially the most populous states like California, Texas, and Illinois constituted a bulk of the casualties that have occurred between the years 2014 and 2022.\nGiven the limitations and biases against populous states, to help provide a more meaningful comparison between states taking the ratio of 2022 to 2014 casualties for each state reveals that each state is experiencing its own unique trend in gun violence. Therefore, suggesting other variables not accounted for in the data set are at play on a by-state basis, suggesting a more nuanced conversation is needed."
  },
  {
    "objectID": "posts/BCB 520 - Midterm Portfolio Post/index.html#conclusions",
    "href": "posts/BCB 520 - Midterm Portfolio Post/index.html#conclusions",
    "title": "BCB 520 - Midterm Portfolio Post",
    "section": "Conclusions",
    "text": "Conclusions\nBetween the years 2014 and 2022, has there been a meaningful change in gun violence in the United States?\n\nOn a nation-wide basis there are more casualties (deaths and/or Injuries), despite less individual gun incidents occurring in 2022 compared to 2014.\nOn a by state basis, the data suggests a few states, for some unknown reason given the available data, are experiencing a surge in gun violence, while other states had less violence and most hover in-between.\n\nIt is important to note the population densities of each state were not accounted for and a 2-fold change in populous state is a lot more casualties than the same fold change in a low population state.\nTherefore, without more data to be more confident on a state by state basis to have a more nuanced conversation, there has been an increase in gun violence in the United States in the form of casualties."
  },
  {
    "objectID": "posts/MarksChannels/index.html",
    "href": "posts/MarksChannels/index.html",
    "title": "ASSIGNMENT 4",
    "section": "",
    "text": "Every time you create a figure, it needs a caption. The text in that section of your assignment should also briefly describe the data set you are using, especially the attributes used for the visualization.\nIn addition, make sure the visualization task actually requires the particular concept.\nFor example, don’t just make a scatterplot with one red dot for the Popout exercise.\nYou need to describe a task that requires we IDENTIFY that point.\n\n\nCode\nstack <- read.csv(\"2022-03-21_trim-stacked.csv\")\n\nhist(stack$logFC)\n\n\n\n\n\nCode\nhist(stack$logFC, breaks = 1000)\n\n\n\n\n\nThe goal of figure 1 is to summarize the distribution of log fold changes observed across all mutant samples. A description of the results would be the majority of proteins detected and quantified did not have its steady-state levels impacted by at least one mutant condition. Using only this figure, it appears that only a small percentile of proteins detected had there steady-state levels impacted in at least one mutant condition.\nA limitation of histograms is it does not show the precise number of proteins with log fold changes between 1.5 and 1.7 for example. In an attempt to answer this pressing question, the second figure has an increased number of bins to narrow in the 1.5-1.7 log fold change region, but I still do not know the exact answer.\n\n\nCode\np350 <- read.csv(\"350_matrix_LogFC.csv\")\n\nplot(p350$R46G, p350$A583T)\n\n\n\n\n\nCode\ncolors <- colorRampPalette(c(\"red\", \"blue\"))\n\np350$color.order <- findInterval(p350$A583T, sort(p350$A583T))\n\nplot(x = p350$R46G[order(p350$R46G)],\n     col = colors(nrow(p350))[p350$color.order])\n\n\n\n\n\nThe goal of these figures is to display if these two mutants (R46G vs A583T) affect the same proteins/genes similarly, correlation suggests these mutants are doing similar things in vivo.\nInstead of position,I used hue for A583T mutant logFC numbers. Solid red is lowest LogFC in the A583T mutation, with purple representing Log FC numbers near zero (Wild-type levels) and Blue is the highest log FC values.\n\n\nCode\nplot(stack$logFC[stack$P.Value <= .055 & stack$contrast == \"WTHsc82_vs_R46G\"], \n     log(stack$Half.life[stack$P.Value <= .055 & stack$contrast == \"WTHsc82_vs_R46G\"]))"
  },
  {
    "objectID": "posts/Spatial Practice/index.html",
    "href": "posts/Spatial Practice/index.html",
    "title": "Practice with Spatial Data",
    "section": "",
    "text": "In this assignment, we’ll consider some of the tools and techniques for visualizing spatial data. Spatial data comes in two broad categories, geographic and spatial fields. Let’s practice a few visualizations to get a feel for how these things work!"
  },
  {
    "objectID": "posts/Spatial Practice/index.html#geographic-maps",
    "href": "posts/Spatial Practice/index.html#geographic-maps",
    "title": "Practice with Spatial Data",
    "section": "GEOGRAPHIC MAPS!",
    "text": "GEOGRAPHIC MAPS!\nIn class I bet Ronald that he would end up creating some kind of map based visualization before he graduated with his PHD. This is because he works on Malaria - a terrible disease with a strong spatial component to its risk levels. Let’s get some Malaria data and map it!\nThe data I obtained were from the Malaria Atlas. I downloaded a csv for 10 years of data for all the countries the had on file.\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(dplyr)\n\nMalaria <- read.csv(\"National_Unit_data.csv\")\n\nIncidence<- Malaria%>%\n  filter(Metric == \"Infection Prevalence\" & Year == \"2019\")%>%\n  mutate(Prevalence = Value)%>%\n  select(c(ISO3, Prevalence))\n\n\nNow I’m going to use the rnaturalearth package to create contry polygons. Then I’ll add the Malaria data to that data frame.\n\n\nCode\nworld_map <- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nmap_data <- world_map %>%\n  left_join(Incidence, by = c(\"iso_a3\" = \"ISO3\"))\n\n\nNow I will make a plot!\n\n\nCode\nggplot() +\n  geom_sf(data = map_data, aes(fill = Prevalence)) +\n  scale_fill_gradient(low = \"white\", high = \"red\", na.value = \"gray\", name = \"Malaria Prevalence\") +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Malaria Prevalence by Country\")\n\n\n\n\n\nOh SNAP! We made a MAP!\nHey! That rhymes!\nWhat is missing? Units? Is that actually prevalence? We sure left a lot of data on the table? Can we add some kind of time thing? Change the variable?"
  },
  {
    "objectID": "posts/Spatial Practice/index.html#spatial-fields",
    "href": "posts/Spatial Practice/index.html#spatial-fields",
    "title": "Practice with Spatial Data",
    "section": "SPATIAL FIELDS",
    "text": "SPATIAL FIELDS\nSpatial field data refers to data that has a continuous spatial distribution and can be measured at any location within the defined area. Here are some interesting examples of spatial field data:\n\nAir temperature: Air temperature data collected from weather stations or remote sensing technologies can be used to create temperature maps or to study climate change, urban heat islands, and other environmental phenomena. TROPICAL CYCLONE!\nPrecipitation: Rainfall, snowfall, or other forms of precipitation data collected from weather stations or satellites can be used to study the hydrological cycle, flood risk, droughts, or water resource management.\nSoil moisture: Soil moisture data collected from in situ sensors or remote sensing technologies can be used to study agricultural productivity, irrigation management, droughts, and land degradation. Elevation data (Digital Elevation Models, DEMs):\nElevation data collected from satellite-based radar, LiDAR, or photogrammetry can be used to study topography, watershed delineation, flood risk, landslides, or geomorphology.\nVegetation indices: Indices like the Normalized Difference Vegetation Index (NDVI) or Enhanced Vegetation Index (EVI) derived from satellite imagery can be used to study vegetation health, land cover change, deforestation, agricultural productivity, and carbon sequestration.\nAir quality: Data on air pollutants like PM2.5, PM10, NO2, SO2, O3, and CO collected from ground-based monitors or satellites can be used to study the impact of pollution on human health, urban planning, or environmental policy.\nOceanographic data: Sea surface temperature, salinity, and chlorophyll-a concentration data collected from buoys, ships, or satellites can be used to study ocean currents, climate change, or marine ecosystems. OCEAN CURRENTS!\nPopulation density: Spatially explicit population density data can be used to study urbanization, migration patterns, infrastructure planning, or public health.\nLand use and land cover: Land use and land cover data collected from satellite imagery can be used to study urban growth, deforestation, habitat fragmentation, or landscape ecology.\nSeismic activity: Spatial distribution of earthquakes and their magnitudes can be used to study tectonics, fault zones, seismic hazards, or infrastructure resilience. DARK NIGHTS IN ANTAKYA\nSPORTS! Let’s check out a baseball example!\n\n\n\nCode\n# install.packages(\"baseballr\")\nlibrary(remotes)\n\n\nWarning: package 'remotes' was built under R version 4.1.3\n\n\nCode\n# install_github(\"bayesball/CalledStrike\")\n\n\n\n\nCode\nlibrary(CalledStrike)\n\n\nLoading required package: shiny\n\n\nWarning: package 'shiny' was built under R version 4.1.3\n\n\nLoading required package: baseballr\n\n\nWarning: package 'baseballr' was built under R version 4.1.3\n\n\nLoading required package: mgcv\n\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.8-38. For overview type 'help(\"mgcv-package\")'.\n\n\nLoading required package: metR\n\n\nWarning: package 'metR' was built under R version 4.1.3\n\n\n\nAttaching package: 'metR'\n\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\n\nCode\n#ShinyDemo()\n\n\nThis example is the Pitch_Locations example from byaesball’s CalledStrike Github Repository\n\nIntroduction\nThere are three functions for visualizing pitch locations.\n\nThe function location_compare() will graph the pitch location for a data frame or list of data frames.\nThe function location_count() will show the locations of pitches for a specific pitcher on a particular count.\nThe function location_count_compare() will graph the pitch locations for a specific pitcher for several values of the count.\n\n\n\nData\nThe package includes the dataset sc_pitchers_2019 that contains Statcast data for 20 pitchers for the 2019 season.\n\n\nPitch Locations for a List\nSuppose we want to compare the locations of the fastballs thrown by Aaron Nola and Trevor Bauer.\nI find the subset of data I need and then create a list dividing the data by pitcher.\n\n\nCode\nd <- filter(sc_pitchers_2019, \n            pitcher %in% c(605400, 545333),\n            pitch_type == \"FF\")\nds <- split(d, d$pitcher)\nnames(ds) <- c(\"Bauer\", \"Nola\")\n\n\nNow we can construct the graph.\n\n\nCode\nlocation_compare(ds)\n\n\nWarning: Removed 22 rows containing non-finite values\n(`stat_density2d_filled()`).\n\n\n\n\n\n\n\nPitch Locations for a Specific Count\nSuppose we want to look at the locations of Aaron Nola’s pitches on a 0-0 count. I can find Nola’s MLBAM id number by use of the chadwick dataset (also included in the package) that contains the id numbers for all players.\n\n\nCode\nchadwick %>% \n  filter(name_last == \"Nola\", name_first == \"Aaron\")\n\n\n  name_first name_last key_mlbam\n1      Aaron      Nola    605400\n\n\nTo produce the graph, type\n\n\nCode\nlocation_count(sc_pitchers_2019, \n               605400, \"Aaron Nola\", \"0-0\")\n\n\nWarning: Removed 4 rows containing non-finite values\n(`stat_density2d_filled()`).\n\n\n\n\n\n\n\nPitch Locations Across a Group of Counts\nSuppose we want to compare Nola’s pitch locations across the counts “0-0”, “1-0”, “0-1”, “0-2”\n\n\nCode\nlocation_count_compare(sc_pitchers_2019, \n               605400, \"Aaron Nola\", \n               \"R\", \"Offspeed\", \n               c(\"0-0\", \"1-0\", \"0-1\", \"0-2\"))\n\n\nWarning: Removed 4 rows containing non-finite values\n(`stat_density2d_filled()`).\n\n\n\n\n\nEND"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\nIn this blog I hope to enlighten the world about the Hsp90 protein and how it impacts your cells!"
  },
  {
    "objectID": "posts/Final Post/index.html",
    "href": "posts/Final Post/index.html",
    "title": "BCB 520 - The Final Portfolio Post",
    "section": "",
    "text": "The focus of my research is the study of Hsp90 molecular chaperone. Over the decades we come to realize this chaperone sits as a major hub of protein-protein interactions (CITE GOPINATH) in which it regulates the folding, activity, and degradation of its clients.\nThe current study expands the proteomic analysis previously done by us (CITE GENETICS PAPER), here we use quantitative proteomic analysis to compare extracts from yeast expressing wild-type yeast Hsp90 to nine different mutants. Our results demonstrate that the mutants elicit varied proteomic responses, even though the cells were harvested at temperatures that do not significantly affect growth. Overall, these results suggest:\n\nIt is possible to selectively inhibit Hsp90 function in vivo, and\nIdentifies cellular processes and/or likely clients that may be differentially affected by Hsc82 mutation."
  },
  {
    "objectID": "posts/Final Post/index.html#creating-figure-1-basic-static-network",
    "href": "posts/Final Post/index.html#creating-figure-1-basic-static-network",
    "title": "BCB 520 - The Final Portfolio Post",
    "section": "Creating Figure 1: Basic static ‘network’",
    "text": "Creating Figure 1: Basic static ‘network’\n\n\nCode\nlibrary(igraph)\n\n\nWarning: package 'igraph' was built under R version 4.1.3\n\n\n\nAttaching package: 'igraph'\n\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\n\nThe following object is masked from 'package:base':\n\n    union\n\n\n\nNODES\nSince our objective is to construct a protein interaction network, intuitively our nodes will be our significant hits found in our study. However, we must also include our experimental variables, in this case single point mutations in the Hsp90 chaperone, this will help promote separation/clustering between nodes later when we apply our measurements from DIA-MS.\n\n\nCode\n# Visualize 350 nodes corresponding to the significant hits in our data\nnodes <- unique(c(proteomic$contrast, proteomic$Gene))\n\ng <- make_empty_graph()\ng <- add_vertices(g, length(nodes))\nV(g)$name <- nodes\n\nplot(g, \n     margin = -.4,\n     vertex.size = 4,\n     vertex.label.cex = .5\n     )\n\n\n\n\n\nNow we must establish the relationship between these nodes. Starting with the relationship between our protein hits and experimental variables (Hsp90 mutants). These proteins were found to have a significant change in abundance when at least one mutant Hsp90 is expressed as the sole source of Hsp90.\n\nColor the edges of the protein-mutant relationship based on increase or decrease (green / red;consider color blind plebs though.)\nEdge thickness can be p value of measurement and/or\nedge color opacity/transparency for p value\nEdge length will be Log base 2 fold change; this will pull proteins nodes towards mutants that have significant impact on said protein\n\n\n\nCode\nedges <- data.frame(\n  from = proteomic$Gene,\n  to = proteomic$contrast,\n  LogFC = proteomic$logFC,\n  p = proteomic$P.Value\n  \n)"
  }
]